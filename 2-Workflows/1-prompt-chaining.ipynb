{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd78270",
   "metadata": {},
   "source": [
    "### Prompt Chaining\n",
    "Prompt chaining is a technique in NLP where multiple prompts are sequenced to gether to guide a model through a complex task or reasing process. Instead of relaying on single prompt to achieve a desired outcome, prompt chaining breaks the task into smaller, managable steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd010d62",
   "metadata": {},
   "source": [
    "#### How Prompt Chaining Works with LangGraph\n",
    "1. Define the Task: Start by breaking down the problem into smaller sub-tasks. For example, if you want to generate a detailed report, you might split it into steps like \"gather data,\" \"analyze data,\" and \"write summary.\"\n",
    "\n",
    "2. Create Nodes: Each sub-task becomes a node in the LangGraph structure. A node could be a prompt that instructs the model to perform a specific action, such as \"List key facts about X\" or \"Summarize the following text.\"\n",
    "\n",
    "3. Establish Edges: Edges define the sequence and dependencies between nodes. For instance, the output of the \"gather data\" node flows into the \"analyze data\" node, ensuring the model has the necessary context to proceed.\n",
    "\n",
    "4. Execute the Graph: LangGraph processes the nodes in order, passing information along the edges. The model generates responses step-by-step, refining the output as it progresses through the chain.\n",
    "\n",
    "5. Iterate if Needed: LangGraph supports conditional logic and loops, so you can revisit earlier nodes or adjust the flow based on intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm= ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "result=llm.invoke(\"Hello\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75070ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "#Graph State\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic:str\n",
    "    story:str\n",
    "    improved_story:str\n",
    "    final_story:str\n",
    "\n",
    "#nodes\n",
    "\n",
    "def generate_story(state:State):\n",
    "    msg=llm.invoke(f\"write a one sentence story about {state[\"topic\"]}\")\n",
    "    return{\"story\":msg.content}\n",
    "\n",
    "def check_conflict(state:State):\n",
    "    if \"?\" in state[\"story\"] or \"!\" in state[\"story\"]:\n",
    "        return \"Fail\"\n",
    "    return \"Pass\"\n",
    "def improved_story(state:State):\n",
    "    msg=llm.invoke(f\"Enhance this story premise with vivid details: {state[\"story\"]}\")\n",
    "    return{\"improved_story\":msg.content}\n",
    "def polish_story(state:State):\n",
    "    msg=llm.invoke(f\"Add an unexpected twist to this story premise:{state[\"improved_story\"]}\")\n",
    "    return{\"final_story\":msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the graph\n",
    "\n",
    "graph=StateGraph(State)\n",
    "\n",
    "graph.add_node(\"generate\",generate_story)\n",
    "graph.add_node(\"improve\",improved_story)\n",
    "graph.add_node(\"polish\",polish_story)\n",
    "\n",
    "#define the edges\n",
    "\n",
    "graph.add_edge(START,\"generate\")\n",
    "graph.add_conditional_edges(\"generate\",check_conflict,{\"Pass\":\"improve\",\"Fail\":\"generate\"})\n",
    "graph.add_edge(\"improve\",\"polish\")\n",
    "graph.add_edge(\"polish\",END)\n",
    "\n",
    "#compile the graph\n",
    "\n",
    "compiled_graph=graph.compile()\n",
    "\n",
    "graph_image=compiled_graph.get_graph().draw_mermaid_png()\n",
    "display(Image(graph_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1de42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"topic\":\"Agentic AI Systems\"}\n",
    "result=compiled_graph.invoke(state)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
