{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f375715",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53b908a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31546695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8138177a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/introduction/', 'title': 'What is Azure Databricks? - Azure Databricks | Microsoft Learn', 'description': 'Learn what is the Azure Databricks Databricks Data Intelligence Platform.', 'language': 'en-us'}, page_content=\" \\n\\n\\nWhat is Azure Databricks? - Azure Databricks | Microsoft Learn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSkip to main content\\n\\t\\t\\n\\n\\t\\t\\tSkip to Ask Learn chat experience\\n\\t\\t\\n\\n\\n\\n\\nThis browser is no longer supported.\\n\\n\\t\\t\\t\\t\\t\\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tDownload Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tMore info about Internet Explorer and Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Table of contents \\n\\n\\n\\nExit editor mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nFocus mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\n\\n\\n\\n\\nRead in English\\n\\n\\n\\n\\n\\nAdd\\n\\n\\n\\n\\n\\nAdd to plan\\n\\n\\n\\n\\n\\nEdit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try signing in or changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nWhat is Azure Databricks?\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSummarize this article for me\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tIn this article\\n\\t\\t\\t\\n\\nAzure Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure for you.\\n\\n\\n\\nAzure Databricks uses generative AI with the data lakehouse to understand the unique semantics of your data. Then, it automatically optimizes performance and manages infrastructure to match your business needs.\\nNatural language processing learns your business's language, so you can search and discover data by asking a question in your own words. Natural language assistance helps you write code, troubleshoot errors, and find answers in documentation.\\nManaged open source integration\\nDatabricks is committed to the open source community and manages updates of open source integrations with the Databricks Runtime releases. The following technologies are open source projects originally created by Databricks employees:\\n\\nDelta Lake and Delta Sharing\\nMLflow\\nApache Spark and Structured Streaming\\nRedash\\nUnity Catalog\\n\\nCommon use cases\\nThe following use cases highlight some of the ways customers use Azure Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\nBuild an enterprise data lakehouse\\nThe data lakehouse combines enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, providing access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is a data lakehouse?.\\nETL and data engineering\\nWhether you're generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models for efficient discovery and use. Azure Databricks combines the power of Apache Spark with Delta and custom tools to provide an unrivaled ETL experience. Use SQL, Python, and Scala to compose ETL logic and orchestrate scheduled job deployment with a few clicks.\\nLakeflow Spark Declarative Pipelines further simplifies ETL by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate data delivery to your specifications.\\nAzure Databricks provides tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.\\nMachine learning, AI, and data science\\nAzure Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning.\\nLarge language models and generative AI\\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers that allow you to integrate existing pre-trained models or other open source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. Integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Azure Databricks, customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and start training with your own data for more accuracy for your domain and workload.\\nIn addition, Azure Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See Apply AI on data using Azure Databricks AI Functions.\\nData warehousing, analytics, and BI\\nAzure Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as SQL warehouses, allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the SQL query editor or in notebooks. Notebooks support Python, R, and Scala in addition to SQL, and allow users to embed the same visualizations available in legacy dashboards alongside links, images, and commentary written in markdown.\\nData governance and secure data sharing\\nUnity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Azure Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.\\nUnity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See What is Unity Catalog?.\\nThe lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of Delta Sharing.\\nDevOps, CI/CD, and task orchestration\\nThe development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Azure Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.\\nJobs schedule Azure Databricks notebooks, SQL queries, and other arbitrary code. Databricks Asset Bundles allow you to define, deploy, and run Databricks resources such as jobs and pipelines programmatically. Git folders let you sync Azure Databricks projects with a number of popular git providers.\\nFor CI/CD best practices and recommendations, see Best practices and recommended CI/CD workflows on Databricks. For a complete overview of tools for developers, see Develop on Databricks.\\nReal-time and streaming analytics\\nAzure Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Lakeflow Spark Declarative Pipelines and Auto Loader. See Structured Streaming concepts.\\nOnline transactional processing\\nLakebase is an online transactional processing (OLTP) database that is fully integrated with the Databricks Data Intelligence Platform. This fully managed Postgres database allows you to create and manage OLTP databases stored in Azure Databricks-managed storage. See What is Lakebase?.\\n\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\t\\t\\t\\t\\tWas this page helpful?\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tAdditional resources\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLast updated on \\n\\t\\t2025-11-05\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn this article\\n\\n\\n\\nWas this page helpful?\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n\\nen-us\\n\\n\\n\\n\\n\\n\\n\\n\\nYour Privacy Choices\\n\\n\\n\\n\\n\\n\\n\\nTheme\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Light \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Dark \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High contrast \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Disclaimer\\n\\nPrevious Versions\\n \\nBlog\\n \\nContribute\\n\\nPrivacy\\n\\nTerms of Use\\n\\nTrademarks\\n\\nÂ© Microsoft 2026\\n\\n\\n\\n\\n\")],\n",
       " [Document(metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/data-engineering/', 'title': 'Data engineering with Databricks - Azure Databricks | Microsoft Learn', 'description': 'Learn about engineering data pipelines with Databricks.', 'language': 'en-us'}, page_content=' \\n\\n\\nData engineering with Databricks - Azure Databricks | Microsoft Learn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSkip to main content\\n\\t\\t\\n\\n\\t\\t\\tSkip to Ask Learn chat experience\\n\\t\\t\\n\\n\\n\\n\\nThis browser is no longer supported.\\n\\n\\t\\t\\t\\t\\t\\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tDownload Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tMore info about Internet Explorer and Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Table of contents \\n\\n\\n\\nExit editor mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nFocus mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\n\\n\\n\\n\\nRead in English\\n\\n\\n\\n\\n\\nAdd\\n\\n\\n\\n\\n\\nAdd to plan\\n\\n\\n\\n\\n\\nEdit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try signing in or changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nData engineering with Databricks\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSummarize this article for me\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tIn this article\\n\\t\\t\\t\\n\\nDatabricks provides Lakeflow, an end-to-end data engineering solution that empowers data engineers, software developers, SQL developers, analysts, and data scientists to deliver high-quality data for downstream analytics, AI, and operational applications. Lakeflow is a unified solution for ingestion, transformation, and orchestration of your data, and includes Lakeflow Connect, Lakeflow Spark Declarative Pipelines, and Lakeflow Jobs.\\nLakeflow Connect\\nLakeflow Connect simplifies data ingestion with connectors to popular enterprise applications, databases, cloud storage, message buses, and local files. See Lakeflow Connect.\\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nManaged connectors\\nManaged connectors provide a simple UI and a configuration-based ingestion service with minimum operational overhead, without requiring you to use the underlying pipeline APIs and infrastructure.\\n\\n\\nStandard connectors\\nStandard connectors provide the ability to access data from a wider range of data sources from within your pipelines or other queries.\\n\\n\\n\\nLakeflow Spark Declarative Pipelines (SDP)\\nLakeflow Spark Declarative Pipelines, or SDP, is a declarative framework that lowers the complexity of building and managing efficient batch and streaming data pipelines. Lakeflow SDP extends and is interoperable with Apache Spark Declarative Pipelines, while running on the performance-optimized Databricks Runtime. SDP automatically orchestrates the execution of flows, sinks, streaming tables, and materialized views by encapsulating and running them as a pipeline. See Lakeflow Spark Declarative Pipelines.\\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nFlows\\nFlows process data in pipelines. The flows API uses the same DataFrame API as Apache Spark and Structured Streaming. A flow can write into streaming tables and sinks, such as a Kafka topic, using streaming semantics, or it can write to a materialized view using batch semantics.\\n\\n\\nStreaming tables\\nA streaming table is a Delta table with additional support for streaming or incremental data processing. It acts as a target for one or more flows in pipelines.\\n\\n\\nMaterialized views\\nA materialized view is a view with cached results for faster access. A materialized view acts as a target for pipelines.\\n\\n\\nSinks\\nPipelines support external data sinks as targets. These sinks can include event streaming services, like Apache Kafka or Azure Event Hubs, external tables managed by Unity Catalog, or custom sinks defined in Python.\\n\\n\\n\\nLakeflow Jobs\\nLakeflow Jobs provide reliable orchestration and production monitoring for any data and AI workload. A job can consist of one or more tasks that run notebooks, pipelines, managed connectors, SQL queries, machine learning training, and model deployment and inference. Jobs also support custom control flow logic, such as branching with if / else statements, and looping with for each statements. See Lakeflow Jobs.\\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nJobs\\nJobs are the primary resource for orchestration. They represent a process that you want to perform on a scheduled basis.\\n\\n\\nTasks\\nA specific unit of work within a job. There are a variety of task types that give you a range of options that can be performed within a job.\\n\\n\\nControl flow in jobs\\nControl flow tasks allow you to control whether to run other tasks, or the order of tasks to run.\\n\\n\\n\\nDatabricks Runtime for Apache Spark\\nThe Databricks Runtime is a reliable and performance-optimized compute environment for running Spark workloads, including batch and streaming. Databricks Runtime provides Photon, a high-performance Databricks-native vectorized query engine, and various infrastructure optimizations like autoscaling. You can run your Spark and Structured Streaming workloads on the Databricks Runtime by building your Spark programs as notebooks, JARs, or Python wheels. See Databricks Runtime for Apache Spark.\\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nApache Spark on Databricks\\nSpark is at the heart of the Databricks Data Intelligence Platform.\\n\\n\\nStructured Streaming\\nStructured Streaming is the Spark near real-time processing engine for streaming data.\\n\\n\\n\\nWhat happened to Delta Live Tables (DLT)?\\nIf you are familiar with Delta Live Tables (DLT), see What happened to Delta Live Tables (DLT)?.\\nAdditional resources\\n\\nData engineering concepts describes data engineering concepts in Azure Databricks.\\nDelta Lake is the optimized storage layer that provides the foundation for tables in a lakehouse in Azure Databricks.\\nData engineering best practices teaches you about best practices for data engineering in Azure Databricks.\\nDatabricks notebooks are a popular tool for collaboration and development.\\nDatabricks SQL describes using SQL queries and BI tools in Azure Databricks.\\nDatabricks Mosaic AI describes architecting machine learning solutions.\\n\\n\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\t\\t\\t\\t\\tWas this page helpful?\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tAdditional resources\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLast updated on \\n\\t\\t2026-01-23\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn this article\\n\\n\\n\\nWas this page helpful?\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n\\nen-us\\n\\n\\n\\n\\n\\n\\n\\n\\nYour Privacy Choices\\n\\n\\n\\n\\n\\n\\n\\nTheme\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Light \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Dark \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High contrast \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Disclaimer\\n\\nPrevious Versions\\n \\nBlog\\n \\nContribute\\n\\nPrivacy\\n\\nTerms of Use\\n\\nTrademarks\\n\\nÂ© Microsoft 2026\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/data-sharing/', 'title': 'Share data and AI assets securely - Azure Databricks | Microsoft Learn', 'description': 'Learn how to share data and AI assets securely with users outside your organization or on different metastores within your Azure Databricks account, using Delta Sharing, Databricks Marketplace, and Clean Rooms.', 'language': 'en-us'}, page_content=\" \\n\\n\\nShare data and AI assets securely - Azure Databricks | Microsoft Learn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSkip to main content\\n\\t\\t\\n\\n\\t\\t\\tSkip to Ask Learn chat experience\\n\\t\\t\\n\\n\\n\\n\\nThis browser is no longer supported.\\n\\n\\t\\t\\t\\t\\t\\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tDownload Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tMore info about Internet Explorer and Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Table of contents \\n\\n\\n\\nExit editor mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nFocus mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\n\\n\\n\\n\\nRead in English\\n\\n\\n\\n\\n\\nAdd\\n\\n\\n\\n\\n\\nAdd to plan\\n\\n\\n\\n\\n\\nEdit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try signing in or changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nShare data and AI assets securely\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSummarize this article for me\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tIn this article\\n\\t\\t\\t\\n\\nAzure Databricks provides tools to share data and AI assets securely with users outside of your organization or on different metastores within your account. Each feature serves different use cases:\\n\\nDelta Sharing for direct data sharing\\nMarketplace for open data product distribution\\nClean Rooms for privacy-preserving collaboration\\n\\nDelta Sharing\\nDelta Sharing enables you to share data and AI assets with users outside of your organization, regardless of whether they use Azure Databricks. Share tabular and non-tabular data, AI models, views, filtered data, and notebooks using open protocols.\\n\\n\\n\\nTopic\\nDescription\\n\\n\\n\\n\\nDelta Sharing overview\\nLearn about Delta Sharing capabilities and use cases.\\n\\n\\nSet up Delta Sharing for your account\\nConfigure Azure Databricks as a provider to start sharing data with external users.\\n\\n\\nDatabricks-to-Databricks sharing\\nShare data between Azure Databricks accounts using secure, native integration.\\n\\n\\nOpen sharing\\nShare data with users who don't use Azure Databricks using open Delta Sharing protocols.\\n\\n\\nCreate and manage shares\\nCreate shares to organize and control access to your data assets.\\n\\n\\nAccess data shared with you\\nAccess and query data that has been shared with your account as a recipient.\\n\\n\\nUse the SAP BDC connector for Azure Databricks\\nCreate a connection with SAP Business Data Cloud (BDC) to share data between SAP BDC and Azure Databricks.\\n\\n\\n\\nMarketplace\\nMarketplace is an open forum for exchanging data products. Share and discover datasets, notebooks, Solution Accelerators, and machine learning models.\\n\\n\\n\\nTopic\\nDescription\\n\\n\\n\\n\\nDatabricks Marketplace overview\\nLearn about Marketplace capabilities and how to get started as a consumer or provider.\\n\\n\\nList data products (provider)\\nPublish your data products to Marketplace for others to discover and use.\\n\\n\\nManage listings (provider)\\nUpdate and maintain your Marketplace listings.\\n\\n\\nCreate private exchanges (provider)\\nSet up private exchanges to share data products with specific organizations.\\n\\n\\nRequest and access data products (consumer)\\nDiscover and access data products using Unity Catalog.\\n\\n\\nManage requests and installed products (consumer)\\nTrack and manage your data product requests and installations.\\n\\n\\n\\nClean Rooms\\nClean Rooms provides a secure, privacy-protecting environment where multiple parties can collaborate on sensitive data without direct access to each other's underlying data.\\n\\n\\n\\nTopic\\nDescription\\n\\n\\n\\n\\nClean Rooms overview\\nLearn about Clean Rooms capabilities and collaboration workflows.\\n\\n\\nCreate clean rooms\\nSet up a clean room and invite collaborators to work with your data.\\n\\n\\nWork with output tables\\nManage and access the results of clean room analyses.\\n\\n\\nManage clean rooms\\nLearn how to update, monitor, and delete clean rooms.\\n\\n\\nWork with clean rooms as a collaborator\\nAccept invitations and collaborate in clean rooms created by other organizations.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\t\\t\\t\\t\\tWas this page helpful?\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tAdditional resources\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLast updated on \\n\\t\\t2026-01-08\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn this article\\n\\n\\n\\nWas this page helpful?\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tNeed help with this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tWant to try using Ask Learn to clarify or guide you through this topic?\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\n\\n Suggest a fix? \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n\\nen-us\\n\\n\\n\\n\\n\\n\\n\\n\\nYour Privacy Choices\\n\\n\\n\\n\\n\\n\\n\\nTheme\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Light \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Dark \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High contrast \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Disclaimer\\n\\nPrevious Versions\\n \\nBlog\\n \\nContribute\\n\\nPrivacy\\n\\nTerms of Use\\n\\nTrademarks\\n\\nÂ© Microsoft 2026\\n\\n\\n\\n\\n\")]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://learn.microsoft.com/en-us/azure/databricks/introduction/\",\n",
    "    \"https://learn.microsoft.com/en-us/azure/databricks/data-engineering/\",\n",
    "    \"https://learn.microsoft.com/en-us/azure/databricks/data-sharing/\"\n",
    "]\n",
    "\n",
    "\n",
    "#docs = loader.load()\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bf49522",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "## Add these to vector DB\n",
    "\n",
    "vectorstore = FAISS.from_documents(doc_splits, OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "932da3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3fb6fd34-1371-4437-91a5-ff0e8a68e5bd', metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/introduction/', 'title': 'What is Azure Databricks? - Azure Databricks | Microsoft Learn', 'description': 'Learn what is the Azure Databricks Databricks Data Intelligence Platform.', 'language': 'en-us'}, page_content='Edit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try signing in or changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tAccess to this page requires authorization. You can try changing directories.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nWhat is Azure Databricks?\\n\\n\\n\\n\\n\\n\\nFeedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSummarize this article for me\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tIn this article\\n\\t\\t\\t\\n\\nAzure Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure for you.'),\n",
       " Document(id='bae66846-1f27-4395-95ca-b907779108ce', metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/introduction/', 'title': 'What is Azure Databricks? - Azure Databricks | Microsoft Learn', 'description': 'Learn what is the Azure Databricks Databricks Data Intelligence Platform.', 'language': 'en-us'}, page_content='What is Azure Databricks? - Azure Databricks | Microsoft Learn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSkip to main content\\n\\t\\t\\n\\n\\t\\t\\tSkip to Ask Learn chat experience\\n\\t\\t\\n\\n\\n\\n\\nThis browser is no longer supported.\\n\\n\\t\\t\\t\\t\\t\\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tDownload Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tMore info about Internet Explorer and Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Table of contents \\n\\n\\n\\nExit editor mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nFocus mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\n\\n\\n\\n\\nRead in English\\n\\n\\n\\n\\n\\nAdd\\n\\n\\n\\n\\n\\nAdd to plan\\n\\n\\n\\n\\n\\nEdit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint'),\n",
       " Document(id='0e36f00d-2c79-4db9-9667-4d5bdf525edb', metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/data-engineering/', 'title': 'Data engineering with Databricks - Azure Databricks | Microsoft Learn', 'description': 'Learn about engineering data pipelines with Databricks.', 'language': 'en-us'}, page_content='Databricks Runtime for Apache Spark\\nThe Databricks Runtime is a reliable and performance-optimized compute environment for running Spark workloads, including batch and streaming. Databricks Runtime provides Photon, a high-performance Databricks-native vectorized query engine, and various infrastructure optimizations like autoscaling. You can run your Spark and Structured Streaming workloads on the Databricks Runtime by building your Spark programs as notebooks, JARs, or Python wheels. See Databricks Runtime for Apache Spark.\\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nApache Spark on Databricks\\nSpark is at the heart of the Databricks Data Intelligence Platform.\\n\\n\\nStructured Streaming\\nStructured Streaming is the Spark near real-time processing engine for streaming data.'),\n",
       " Document(id='f9271e9f-d67a-4f22-91a7-be0d54b05aac', metadata={'source': 'https://learn.microsoft.com/en-us/azure/databricks/introduction/', 'title': 'What is Azure Databricks? - Azure Databricks | Microsoft Learn', 'description': 'Learn what is the Azure Databricks Databricks Data Intelligence Platform.', 'language': 'en-us'}, page_content='Data warehousing, analytics, and BI\\nAzure Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as SQL warehouses, allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the SQL query editor or in notebooks. Notebooks support Python, R, and Scala in addition to SQL, and allow users to embed the same visualizations available in legacy dashboards alongside links, images, and commentary written in markdown.\\nData governance and secure data sharing')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Databricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e5fa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever to retriever tools\n",
    "\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retriever_vector_db_blog(query: str) -> str:\n",
    "    \"\"\"Search and return information about Azure Databricks.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever_tool = retriever_vector_db_blog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f07d3dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='retriever_vector_db_blog', description='Search and return information about Azure Databricks.', args_schema=<class 'langchain_core.utils.pydantic.retriever_vector_db_blog'>, func=<function retriever_vector_db_blog at 0x0000029DA9B040E0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "736def81",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_urls = [\n",
    "    \"https://docs.snowflake.com/en/index\",\n",
    "    \"https://docs.snowflake.com/en/user-guide/data-load-overview\",\n",
    "    \"https://docs.snowflake.com/en/user-guide/data-integration/openflow/setup-openflow-roles-login\"\n",
    "]\n",
    "\n",
    "s_docs = [WebBaseLoader(url).load() for url in snowflake_urls]\n",
    "s_doc_list = [item for sublist in s_docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8e13384",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "s_doc_splits = text_splitter.split_documents(s_doc_list)\n",
    "\n",
    "s_vectorstore = FAISS.from_documents(s_doc_splits, OpenAIEmbeddings())\n",
    "s_retriever = s_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be6effb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever to retriever tools\n",
    "\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retriever_vector_s_db_blog(query: str) -> str:\n",
    "    \"\"\"Search and return information about snowflake.\"\"\"\n",
    "    docs = s_retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in s_doc_list])\n",
    "\n",
    "s_retriever_tool = retriever_vector_s_db_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08b02671",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, s_retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b12ce",
   "metadata": {},
   "source": [
    "## LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9563cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import langchainhub as hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage],add_messages]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34ecf9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user sent \"Hello, Groq!\" which seems like they\\'re greeting me as Groq. First, I need to confirm if they\\'re expecting me to respond as Groq, but I should clarify that I\\'m Qwen. However, since they mentioned Groq, maybe they\\'re testing or want a specific response. I should politely correct them and introduce myself as Qwen while being friendly.\\n\\nNext, I should check if there\\'s any specific request in their message. They just said hello, so probably no further action needed. My response should be concise and welcoming, encouraging them to ask questions or seek help. I need to avoid using technical jargon and keep it natural. Also, make sure the tone is warm and approachable. Let me put that together.\\n</think>\\n\\nHello! It seems there might be a bit of a mix-upâ€”I\\'m Qwen, a language model developed by Tongyi Lab. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 217, 'prompt_tokens': 13, 'total_tokens': 230, 'completion_time': 0.47549463, 'completion_tokens_details': None, 'prompt_time': 0.000350225, 'prompt_tokens_details': None, 'queue_time': 0.159507815, 'total_time': 0.475844855}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2344-9c7c-7aa0-b6b1-4b461163beac-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 217, 'total_tokens': 230})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "llm.invoke(\"Hello, Groq!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b2a89af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    An agent that uses a Groq LLM and can access retriever tools. It will use the tools to answer questions about both Azure Databricks and Snowflake.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including message history.\n",
    "    Returns:\n",
    "        str: The agent's response.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    model=model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "04a83054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state) -> Literal[\"generate\",\"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Grade the documents based on their relevance to the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including message history.\n",
    "    Returns:\n",
    "        Literal[\"generate\",\"rewrite\"]: The action to take based on the grading.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELAVANCE---\")\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "        binary_score: str = Field(description='If the documents are relevant, respond with \"yes\". If they are not relevant, respond with \"no\".')\n",
    "\n",
    "    #LLM \n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "    #prompt\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "  #chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages= state[\"messages\"]\n",
    "    last_message=messages[-1]\n",
    "\n",
    "    question=messages[0].content\n",
    "    docs=last_message.content\n",
    "\n",
    "    scored_results = chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "    score = scored_results.binary_score\n",
    "    if score.lower() == \"yes\":\n",
    "        print(\"--- Documents are relevant----\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"--- Documents are not relevant----\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b126679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate a response using the agent.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including message history.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- GENERATE RESPONSE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question=messages[0].content\n",
    "    last_message=messages[-1]\n",
    "    docs=last_message.content\n",
    "\n",
    "    #prompt\n",
    "    # prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "\n",
    "\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1f53d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Rewrite the user's query to be more specific.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including message history.\n",
    "    Returns:\n",
    "        str: The rewritten query.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- REWRITE QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question=messages[0].content\n",
    "    \n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\" \\n \n",
    "            Look at the input and try to reason about the underlying semantic intent \\n \n",
    "            Here is the initial question: \n",
    "            \\n ----- \\n \n",
    "            {question}\n",
    "            \\n ----- \\n\n",
    "            Formulate an improved question:\n",
    "            \"\"\")\n",
    "    ]\n",
    "\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c5c4e42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAHICAIAAAAN8PI9AAAQAElEQVR4nOydB2AT9RfH3yXpnrTQllKgtOwhe4rsqQgiyJYtIKDIEgeIgP6R7cAFCAiCgEzZe5UpmzILpYsuaEv3SJP7v+TaUNIk7bVNcpe8DzVe7ncjudz3fu+93+/3fjKWZYEgCD7IgCAInpBsCII3JBuC4A3JhiB4Q7IhCN6QbAiCNxYom6vHXkQ/ycxMVeTKWXmWkgWWAQbXs4zqHyNRbYNrGS7wLmFBqSplGCUwEqUCF1SrNXupNpExylz25Uo8ghJACqDIO6MSWAm3MaPaCE/BKtXv1AsSGShzX348PJ9EKmEVL9dIbUEqk9raSzy8beq1cPPytwVC2DAW025zYG3s08cZOdmsTMbY2ktt7BiplJFnKxgJy+YJQ/Vl1bJhVApQf3FGyrAKbkH1wqpUpj4cC/mqAakNKOSqBe5QjES1mUZLatRyUZ8D1KfIl416SxtGKX95kVF5EkneSTlkdhIly8jTFZmZKjHhLp4+9m17V/CrZQeEILEE2ez8KTo2PNPBWVq1rlOn9ypobneRciso5e6F5ITYbAdnWa/Rlbyq2gAhMMQtm/uXU09uj3ctZ9tzlK+HjxQsi31/xIbdSfOp7NB/aiUghISIZbN/TWzEw/SO7/nUbu4Elsv6+eE5mYpxCwOAEAxilc2tsyn/HU4c840/WAEH18Y/fZIxdoE/EMJAlLLZ+cvTpGi5lWiG4/jf8SE30iYsojpHEEhAbATtSkqIyrEqzSCdB3tVre20dm4YEAJAfLK5EZQ4ZkE1sD56jvLG6PTe32OAMDcik83ar8Mr13CQWFrMrLiMnlctMiQDFECYFzHJ5t6ltOz03D4TfMGK8a7isGlJBBBmRUyyuXgowTfAkmPNxaHfZN+k+BwgzIqYZJOeLO81xgdMyOPHj3v16gX8+eyzz/bs2QPGQAJObrK9q2KBMB+ikc2xTfF29lKpaXs53r17F0pEiXcsDpVrOsWEZwJhPkQjm5jwLLcKxuqdlZqaumTJkj59+rzxxhvjx4/fvXs3rvztt9/mzZsXGxvbrFmzTZs24ZqtW7dOnjy5Q4cO3bt3//zzz6Oiorjdt2zZgmtOnTrVokWLpUuX4vbR0dELFizALcEINO/kmZutBMJ8iEY2mWm5Fas5gHFAedy6dQuVsH379vr16y9cuBDfTpgwYfjw4T4+PleuXBk6dOiNGzdQWg0bNkRh4PaJiYmzZ8/mdre1tU1PT8d958+fP2DAgHPnzuHKOXPmoJDACLh5q4YphN7MAsJMiGa8jULO+vobSzbXrl1DhbRq1QqXP/rooy5duri7u2tt06BBg23btlWpUkUmU100uVw+derU5ORkNzc3hmGysrJGjBjRvHlzLMrOzgYjY2MnjQ3PDGhoD4Q5EI1sWJZ19jDWp23UqNFff/314sWLJk2atG7duk6dOoW3kUqlaJUtW7YsODgY6xZuJdY5KBtuuV69emA6lOkpciDMhGiMNKVq9JexPu3XX389ZMiQCxcuTJs2rWvXrr/++mtubq7WNqdPn8bSunXrrl69+r///lu5cqXWBmiqgSmhvJDmQzS1jUSiCkBXMM4HdnV1HT169KhRo27evHny5Mk//vjDxcVl2LBhBbfZtWsXVkqTJk3i3mIUAcwHq2DsnWn4mtkQTW0jkYKRoq7on2CIDJ0TdFFQGOixYCjs/v37hTfz8vLSvD1x4gSYj1y5soIfDZk2G6KRjYOTNPqJUWJH6OKvWrVq1qxZWNUkJCTs378fNYP6wSIMADx//hwDYuHh4TVr1rx48SJG1dB+4+LRSEyMjo6VdnZ2KDDNxlDW5GSyCoXSsgfnCRzRyMazol1CjFEiVE5OThhZjo+PHzNmDDa/bNiw4ZNPPnn33XexqG3btqifGTNmHD58eOLEiW3atEH3BmMG2JiDMWj0cz7++ONDhw4VPiaafOj/TJ8+PTOz7GvISwcTZDbi67puSYhmmFpaEvvnt6GTlgaC1bNubpiTm3TAtMpAmAnRPLScyzEyGbN/LfXFgrQUeZch3kCYDzGlF3ztdbfrp5MMbICN9Po8dfQxuGbKwmD02Ui9YBADRzbwkbBdtWD4oSC7f3lq7yj18KEUhOZEZLkEfpsVWqOhc+chum+ppKQkfb4Ettyjp66zyMPDw97eWM3t0dHR+ooMfCRvb29sXdVZ9PP0R30nVvYNpDCaORGZbKIf5+z8OWLy8upglfy9KBIkzOCZfkCYFZEFZHwDbavUtNJMFJcOJSYnykkzQkB8cczeEyrK7JgtS6PAmkh9xl45lkQJnwSCWNML7l8T+zw6e8RXVcEKeHA1/cSW2A+XUPBdKIg4me2m7yKy0hUWn/xp10/R0RGZk0gzQkLcqdOPbIx/eCOlcg2nPhMqgsVx62TKuYPPbG0lY76xxrxwQkb0E3UocmDDwrCMFAU2ZbTp5VW1jiVEZg+vj39yL41VsHVfd2/f1xMIgWEh00JFPsg+vSsuJSGXYcDeSeLoKnN2w7ZEJif7lVR8eROb5U/iJJVJFLkvB+XLbJjc/PmbVDM3qWdoyl9mNTM95U0ppZ7dSclNKSXJm0aKgbwtJRLVrGq4u1TGKHK5aackrEIplalmjFIqufmj8IySXLlSZgusQpKTxT6PycrOUCiVrL2TTY3GLh36kWAEiuXMpsZx50Jq6O205AS5PFuJ3yw785VUFVIJKAqskEhBWXAywPxZ00A9LVqeOPKmSNPMrsbi3c+o2yJRG0pl3sYFJl9T6UG1RqlahTpRcH2gVYJiJFLVhG5cEe4jkSqVCgmeV8Iwtg5SmQ34VXdo924FIISNpcnG2Bw/fvzIkSOLFi0CwoqhmaL5YaAjGWE90B3AD5INASQbvpBsCCDZ8EUul9vYUO4La4dkww+qbQgg2fCFZEMAyYYvJBsCSDZ8Id+GADGOtzEvVNsQQLLhC8mGADLS+EKyIYBkwxeSDQEkG76gbCgkQJBs+EG1DQEkG76QbAgg2fCFZEMAyYYv2NxJsiHoDuAH1TYEkGz4QrIhgGTDF5INASQbvpBsCCDZ8IV6QBNAsuEL1TYEkGz44uHhQbIh6A7gR3Jyck5ODhDWDcmGH1jVoHsDhHVDsuEHygbdGyCsG5INP0g2BJBs+EKyIYBkwxeSDQEkG75gWyeFBAiSDT+otiGAZMMXkg0BJBu+kGwIINnwhWRDAMmGLxQSIIBkwxeqbQgg2fCFZEMAyYYvJBsCaMYBvpBsCKDahi8kGwJhWJYFoijeeuutmJgYXGAYhlujVCr9/Pz27t0LhPVBRlqxGDJkCIaeJRIJkw8ud+3aFQirhGRTLAYMGFC5cuWCa7CqwZVAWCUkm2KBVc3QoUPt7Ow0a1q3bu3j4wOEVUKyKS59+/atVKkSt4yCGTRoEBDWCsmGB8OHD3d0dMSFpk2b+vv7A2GtWGAk7eap1NiozJxMVZhYIsGQl2olIwFWia8Mq2TV6xmlksWVDKgWCqzBQBnLKvMOlbcXo4qgcTtev349IzPttdcauTi7qHfPO37+Drh33lm4U3NHUIEPKGX+qy5ktlIHF1mHPp4gBULgWJRsHl/POr4tGu9bmQ2Tk6m6PTV3Lcuo/nG39cv1uEqlB/XtzuTf0PnbvFyWqF/VK1mVLPBQ6lqaYVWSKiQb1Xo8F6cQzRkZYDVFupDaoNyYnBxl+Yp2A6ZVAkLAWI5sngRnHv4rpnk3r5pNnUHMbF8RWd5X9va4ikAIFQuRTVIcbF36eOjsQLAIdv0c6eQi7feRLxCCxEJCAgfXR3n6OoKl0HWAX3xkJhBCxUJkk5Ys961hObJxrqDqhXDnfCoQgsRCunLm5rB29gxYEBjWS3lBw0gFioXIRqFQ5sotKpKuzGVZhRIIQUIDBwiCNyQbgaJqZrIoq9OiINkIFAmQaoSLhciGUXcAsCRYvb1wCPNjIbJhWbC0rnUsWNxXshzISBMwZKUJFZINQfCGZCNgyEgTKiQbguCN5UTSVENZLAmGItDCxXIiafqGf4kVFiiDnWAhI02gYFUjodpGqFAKDqPz5MnjQUN6AU+wqlFSbSNUqLYxOg8e3gXCsrCgkABPLlw4e+Lk4Vu3r6ekJNepXf/998c2btSMK/p3745t2zampKa0atV2zKiJWFfM/vLbzp26Y9GdO7f+3LDq/v07bu7lWrd6Y8TwcU5OTrh+3vzPGIbp0rnnd4u/zszMqFu3wYRxU+rUqb9u/W8bNq7BDTp2bva/b1a0bv1GMT8egxEOKdkCAsVCfhi+3nNWVta3C2dnZ2d/Nmve/779vkoV/y9nT01MTMCie/fvrPh+Yfv2XTb+ubNDuy7zv/kcVOmgVBcq6mnkjE8nZmVnrfxp3YJ5S0NDQ6ZOG8dNQCCTye7cvXX02IHfft14cH+Qna3dwkVzcf2okRMGDRzu7e1z8viV4mtG/Y0YoPE2QsVKn2f29vZrVm2ZPu1LrGHwb8L4TzIzM28H38CiI0f2eXh44u3u5ubepk275s1aafY6duygjcwGBYMy8/cPmDF9TsijB0HnTnGlmRkZM2d85VuxEkqoc6cekZHhGRkZUGIY6lwjXCxFNvzbbTIy0n9auaT/gB5oPvV8qy2uefEiCV9DnzxC4wpvfW6zdm901uxy587N2rXroZy4tz4+FX19/dDM495WruLP5exEnJ1d8DU1NQVKDHXlFDCWEhLg2W4TFxc7ZerYJo1bzPnyf+iHoFvStXterZKWlurl9TInukYkXNH9B3dRZgUPlaQ27SDfkCOsASvtJXDq9NGcnBx0bBwcHCC/nuGws7PPLTCFekLic82yh2f5Bg0aof1W8FBuru5gDFCDEqpuBIqV9hLA6JmLiyunGeT0meOaokqVKoeE3Ne8PZfvuiCBATWOHN3f8LUmmoolLCzUz68KGAMMByjJuREoVmpXBATUSEh4joFmjINdunz+2rXLaIzFx8di0ett2oeHP9n893qWZf+7cvH27Ruavfr3H6pUKlf+sgwDcejx/77qx9FjB6IvZPhcqCs8V1DQqefPnwEvSDVCxUplg40w7w8bs2HjanRpduzY/PFHn3bt8iZKZfmK/7V7o1PfdwZg40zffl137d46duxkUE8Lha+uLq5/rNnqYO8w/sNhw0f2u3Hz6swZc2rWqG34XK1atm1Qv9GcuTO4SB0PyEYTKhaSA/qnaY9adKtQt7UblBqsf9D0ql69JvcWm3EmThqx+vfNmjWmYcO8x407urV5uzwQwsNSapuy6y+MdcIH44f88OOi2NiYu3dv//DDd/XqvRYYWANMC8Nwk+0QQoT6pGmDrZ/YDHrw0L+jxw7A5pdmTVtNmPAJY/KxLyybNxEVIUBINjro9VZf/APzQr0EBAzJRqhQLwEBY0HNndQ4SJgKC2rutLDGQRrdKWDISBMslEpAuJBsBApjgXmtLQfLkY2lpU5X5RKg+kagWI5sLO0WU1U3zIkTJ+RyeXZ2dkZGBr6m0tP7HwAAEABJREFUqZk1axYQZoWMNKHCwrat24Jjd3KyQUAVMFR1htqzZ8/58+eBMB80skq4dOrc0cbGBqsXVI5EDddZgTRjdkg2wsXTs/xHH33k6upacKW9vT0Q5sZCZGNjx9hZ1u1kYyeR2Ul69uzZp08fW1tbzXqpVLpw4cKoqCggzIelyMZWFh+ZAxaEQqGsWscZF6ZMmdKyZUtufAdq5uzZszVr1pw8efL06dNv3OA5gIcoIyxENn7VHZ4+LkV2JYFx5Uiija3Eu3JeJbNixYqAgAClUunjo8oN0q9fv927d/fu3XvlypUjR448duwYEKbFQoapIX98Fe7mbtt9TEUQP5u+fdJrTEW/Wq/YnV26dCmskDt37mzcuDE4OHjYsGGDBg0CwiRYiGwSExPxphnWYZUyh0HbxrOSg0KR+7JYFbiFl7PiYjwKvzX3qm4nZfNju5D/nlupKVNvzy2/+gp5B1H/X7UdNrew6pOpe/4zqvznXENswe01o3fYl6USqSQrnQ27m5oUkzn6qwBbZx7tt7GxsX/99df27dvff/991I+bWxmMciUMYCGyuXLlCpoxHh4eB9bFx4RmyHOUuTmvZoJl9DaI5sknf4OCd7XmZtcscxvnbVNQNqrMOap/L0+kWlRfXubVI7Gvfp78BYmUkdpIXNxlQz6uDA5QAnJzc7Hm2bRp0xtvvIHiCQwMBMI4iFs2Dx48QOf46NGjYCpOnjx54MCBJUuWgIDZu3cviqdChQpY+bRo0QKIskbcIYEzZ87s3LkTTAg2m3h5eYGwefvtt7ds2TJkyJA///xz8ODBqHMgyhRR1jaollOnTn311VdAFEVISAi6PefPn+fcHsq4WyaITzZowc+aNWvx4sXYiAEmJyMjIzMz09PTE0RFUlISigc9H1XgZNgw4VeYAkdMskEfxsHBoU2bNmZ8ZO7fv//y5cvz5s0DcbJ582bUT+PGjYcOHVq3bl0gSoRoqmyMlZ04caJt27bmNTNE4dsYAB0edHXat2//3XffjR8/Hs1dIPgjgtoGK5muXbti0wTXRk6UFVevXsWAW3h4OJptffuaO8GVqBC6bNasWRMVFfX111+DMEhLS0Pnyt3dOJNzmAOUDZptR44cGaZGMwsDYQDhyub69etogt+5c6devXogGLZu3RoRETFz5kywLNLT0/9S8+abb2LMzc/PDwj9CNS3+eijjyIjI3FBUJpBnJycRBdGKw74vdDVoe7VxURwtU1cXJyrqyv+Zq1btwbCTJw+fRqj1WiOotnWpUsXIF5FQLLJzs7++OOPsU0mICAAhEpqaqpSqbSSvpLUvVofApLNwYMHMbbbtGlTEDBr167NysqaOHEiWA3Uvbow5vdtEhMT0ZjGhZ49ewpcM6CaOd3Zw8MDrAmM+8+YMSMoKMjR0bFfv37Y1Pv48WOwbsxf28yZMwcNAKG5/oQ+qHs1mFE2GCg7duzYqFGjQFQkJydLJBIXFxewbi5cuICWG1oKKB6MWYOVYR7ZoPePNcyvv/4quob/n376CY374cOHA2HF3atN/T3RLMb4DGp1165dYuwsg8Fx8ok11KhRA12dbdu2YSXcqlWr5cuXx8fHgxVg0toGBbNgwYL169dTjjyLxHq6V5tINo8ePapevfrdu3fFfjVfvHghk8kwngaEHo4cOYLicXBwQPG0a9cOLBFTyGbnzp1Hjx5FTwbEz3fffYf679+/PxAGsezu1cb1bWJiYkDd1mEZmkHc1ABRFNgEt1wNmhjt27dfvXp1ZmYmWApGrG3wknHRfSCsG8vrXm0U2eBlysnJOXjw4JAhQ8CywJYKjGdgezkQ/NmxY8fGjRsDAwNRPI0aNQLRUvayWbRoUb9+/QICAoQZxZfL5VlZWVBSzpw5gw/L0nQ2dXJysvL0MRbQvbqMZYPev0KheO+990CoYE1YGiM7LS3NVg2UFHSNbGxswOoRdffqMpPNDz/8MGXKFLTNSnNLmYBSyqb0kGwKItLu1WVjLXz44Yd16tTBBYFrpvQolUqLmaNBCIi0e3Vpa5vDhw937949Ozvbzs4OxEApa5vk5GRsyCMjzUiIpXt1yWsbdKxbtmzp7++Py2LRTOnRzDtbkIEDB27evBmIUiOW7NUlkQ1WUE+fPsVn9vnz52vVqgUi59tvv8U6s5gbu7i4UF1hbFq3bv3zzz/Pnz//0qVLXbt23bBhA9rGICR4yyY8PBxrT7x7ypUrZ5YszGVOSEhI8Tcm38ZkCLl7NQ/fhouSoffWtm1bEC1avk2PHj24BWxOwcY4yB+AFRkZ6erqig1zkyZN0mSvxSI0HqKjo7WK0Ejr06cPmhZ4MXfv3n306FGsjStXrty0adPhw4drPVzItykZgupeXdza5ty5c1yTv6g1U5g9e/bg69SpUznNXLt2bcGCBdgGh00KX3zxBT7eVq5cyW3JFbVv337dunVaRQWPhqZ53759UV1vvfXWoUOH/vnnHyDKAkFlry5aNljJgLpxCoPrYOmgGf3666/jfY91Aj7Sxo0bd/ny5YcPH2qK0E/19PTUKtJw+/ZtNC3QHHd3d+/Zs+eKFSuaN28ORNnRrVs3/CHw4mOtjgHrXbt2gTkoQjboK69fvx4X8IOCFfDkyZOCQY6aNWuCeqpDTZHGtylYpAHldP36dbTCjxw5kpKS4uvrSxNoGoPC3asVCgWYEEOywRbcixcvWolgQO32aDVAcXnEMzIyNEVpaWncL6QpKngErKYmT5784sUL/EXRqFi8eHFCQgIQxqFq1apffvklWm54wTHsBiZEZqAMW3Dnzp0LVgMnmIIdPTlVeHh4GCgqeARs1empBuONN27cQBcW9SbeOaREAcZy8IIvW7YMTIih2iYqKgpdGrAaZDIZeib37t3TrEEbAF+rVaumKcIYGi4XLCp4BIyhhYWFgfpBiLG1d955hzLxWSSGZHPlyhVzuVwmA6uR8uXLX7169ebNm7m5ub1798Y2XHQ3U1NTcc2qVasaNWpUvXp13JIr2rlzJzotWkUaTp06hdE2tGxxGwwYYPiRJvqzSAwZaX5+fkJrnTUGgwYNwnAzPiMwRIOhZ/RGMGb422+/YZtMkyZNNAkQNUUoGK0iDVOmTMEduUmssDkYjQeM9gBhcYhygvXSUMqunFgL2dvbl6a9kpo7y5zg4GD0bbA9DUwF+Tb8oD5pBJBvwxfqk0YA+TZ8wXYbNNIsfjQeYRhDsmmmBogCWHn2DIKDfBt+ODs7U1VDkG/DD/JtCLBC38bR0bE0Q7iXLFnSvHnzDh06QEkhM88CsDrfhmEYrndMyeAC0KU5AmEBGPr50bdJTk6mWTULws3OS1g55NvwIyEhITU1FQjrxpBs0LehnoharF69uvhpbghLhdpt+FGhQgWaSo0g34YfY8aMAcLqId+GH4mJiSkpKUBYN9QnjR+bNm1yc3MbPnw4EFYM+Tb88PT0pNnhCfJt+GF50yoSJYB8G37gc+TFixdAWDfk2/Bjx44dWVlZEydOBMKKId+GH+XKlTPvHIaEECDfhh99+/YFwuoh34Yf2GiDTTdAWDfUJ40fBw8e/OOPP4Cwbsi34Qf6NtRLgCDfplj06NEjPj6eUYPRxd9++41lWWz6PHr0KBDWB/k2xWLAgAEymYybI1ozWTRVxVYL+TbFAmVTuXLlgmt8fX2px4DVYkg2+DR99913gVDneerdu3fB3B34QGnQoAEQVgnlSSsugwYNqlKlCrdcvnz5gQMHAmGtkG9TXLCq6devn6OjI6irmqZNmwJhrYi+T1rEveyMjGwo9DHRa2dB9c/QevTsC+YKfPWtakvuLQOMkmEZtn7Vbo2qP05NTe3UvN+DK6lYqvssmuMw6rcFS5lX3mJsQane8uW5XkVqI6tay9HWAQhBIeJ2m3++j3oenYO3qDxHyejc4tV7VAPe5gzo2INVSUP3kdh8CdR06wduEHYZ/+L0Hk1z3sKyKd4n0SCzlSiVrIOTdNBUfwc3IASCWNttNi+Jys1me47286xo+RmZz+6IX/9t6Mgvqzq4SYEQAKL0bTZ+GyEFpu9Hla1BM8gb/bwGfxaw7tswIISB+NptQm9npqfI3xxXCawJqRQ8vO3/XhIJhAAQX7vN7XPJDs7WOA1gtbquaYm5QAgA8bXbZKXKrTNnv4uHLFdBg20Fgfh8m6xshVxujTPMYGOAwiq/uAChXAIEwRsab0MQvBGfb4PtmwwDBGFGxOfbqPqgkIVPmBVR+jakGsK8iM+3YSRkpBFmRny+DasEmuGcMC803kY00LNCOFC7jZgg5QgE8fk2EgmTy1jj/cMAkE8nEMTn2yiVegeTEYRpIN+GB336dt6wcQ0QVo/4xtswqrrGWEbakyePBw3ppa904ID3X2vQGAirR3y+DYs2mtGM/AcP7xooHTJ4JBCEGH0biaq5k19tg8bVjh1/T5n6QcfOzVJSVYnPDx3eO3HyyJ5vtcXX7Ts2c1lj1q3/bdHieXFxsbjZP9s3hYY+woWLF4P6D+gxdtxgeNVIu3Pn1qezJvfu0/H9Ee/+8uuK9PR0XPnflYu4S3DwTc2p792/ozrIpXP6diHEiPh8G6WquZNfbWNjY7PvwK7q1WstWfyzo4PjseOHUB41a9Te/Ne/Y8dMQtms/GUZbjZq5IRBA4d7e/ucPH7lvf5DcS9cueGvNWibTZ82u+ABo55Gzvh0YlZ21sqf1i2YtzQ0NGTqtHG5ublNGjd3cXY5c/aEZsugoJO4pnmzVvp2AUKEWEUOaIZhXF3dPpo0o1nTljKZ7MCB3a+91viTKZ+VK+eBN/qoERN2796WlJRYeC98xTseJVSn9ivpe44dO2gjs8G7v0oVf3//gBnT54Q8ehB07pRUKu3YsduZs8c1W6KEOnfugev17QKECBFfLgFJifqk1aqZp39swA2+c7N5s9aaosaNm+PKW7ev69yxZo06hVfeuXOzdu16bm7u3Fsfn4q+vn7cETp06Ipm3sOQ+6AOMERFRXTu1MPwLjygwLseCqbnNgHiy5NWsiCarW1eaqicnBy5XP7H2l/wr+AGhWubvB11/R5paan3H9xFp+WVIyQm4Gujhk2xEjtz5jgagWeDTlao4FW/fkPDuxQTFljqjKeP7OxsMCGGZIO+TXBwsODSC+rMUVts7O3tHR0du3V9q127zgXX+1b0K/5BPDzLN2jQCH2hgivdXFU1CZp2aKeh9YVeEzo2Xbu8WeQuxYSG5wkHkfZJK9X9ExhYMzUttXGjvAc/Vj4xMU+9vLx5HCGgxpGj+xu+1kSSn0QnLCzUzy9vPoJOHbrt3LkFQ3DovXzx+YLi7EKIC/H5NqUf3fnBmMnnzp06cHAPPhRu374xf8Hn02ZMQOMNVE+KKgkJz4OCTkVGhhs4Qv/+Q3FfjL9lZWXhlr+v+nH02IGhTx5xpfXqvYYixHB2QEB19P6LswshLkTYbiNVjVQrDWgsrfpt061b1/v264GMoegAABAASURBVIpB4fT0tG8WLOd8ylYt2zao32jO3BnHTxw2cARXF9c/1mx1sHcY/+Gw4SP73bh5deaMOejMaDbo0L4rRgU6dexe/F0IEcEYcDN3796Nvs3s2bNBSPz5TZgyl+k/tSpYGeF3009ti5m8ojoQr4J36bJly9atWwemQny+DdY2SgUQhBkRX5801AzFYQnzIsI8aZSCgzA3IsyTZrUpOGh4p2AQn28jlTGs0hp1g88LoGGtwkB8vo0il1UqrfHuUZum5NUJAnGOt7HK+W0I4SDO8TaUhYowK+LzbRgpMCQbwqyIcH4bhZWGBAjhID7fBusaCicR5kWMedIYlto7CbMiPt+GpoUizI74fBs7O6lcapU5oKUSqQ1Vs4JAfL6Nk5vMOucZT47LksqoxUoQiM+3adnTMzPNGkcOhN5NL+dl0vwshD7ElyfNq7It3j07f4gEayLqQU5GYs57n/gCIQDEl0sAGTSjkqevzbbl4Q8upYKl8zw65+D66NM7no5bFACEMBBfnjSOXmN9DqyNvX7q+eUj8UqFUiuZOqOKtzH63pYStmD//VfeqDJRMfnvVadkNB9Ad/CPYfP7NLO6BwVIJapIgGs52YRF1YAQDCLMk5bPm6N9VP9TQGamArScHaxElfrfgrpDqGrgjuYtAwV7HhR4e/z48Ws3b8ycNv1lKd7LCqWOHTUCyFtmXg4MkqjnFuHeaj6M1vYvP4wkPTV10KCBa9etq+Dt4+AMhNAQ/9ydUnBwlkJJKNZed0KuN2hU08FNWoJ9S4yDi+v+o7vPnj1bJdAHCOHBUIJUgTNhwoTvv//e3t4eCD2YPnONCPukmZa0tDQwKx9//PHChQuBEBI0d6chLl26NGvWLDAr2AYwb948XNi2bRsQwsAq5rcpMY8ePXr99ddBGFSqVGnEiBFACAARjrcxIUOHDgXBgAKuXl2VkvPevXt16tQBwnyQb2OI8PBwQYVMvL1V0yIkJSVNmzYNCPNBvo1e0EJDx4YR3tieNm3avPPOO/hQo0lzzQX5NnqJjIzs3r07CJJ27drhr4PK+f3334EwOeTb6KVjx44gbGrVqnXmzJmgoKC2bdsCYULIt9HLrVu3TDwjZAn44IMP6tevr1AoTpw4AYSpIN9GN8nJyeh2m3j+4ZLh7u4ulUoPHz584MABIEwC+Ta6efr06ZAhQ0A8LFq0yMdH1YEtLi4OCCNDvo1u6qoBUdGkSRN8Xb58OXplPXr0AMJokG+jm4sXL2LzCIgQrHawqgTCmJBvo5uPP/7Yzc0NxMmYMWPw9ddff7158yYQRoB8Gx1ER0dPmjRJIhF3mpixY8f++OOPWVlZQJQ1NN7GwsEY+t27d2vWrOnk5AQWCo23EQRnz54NDw8HiwBj6IGBgW+++eazZ8+AKCPIt9EBetWWNJrS1dX19OnT8fHxmZmZQJQF5Ntok56ePnjwYK6vsSVRr1499NbeeecdbMkFonSIMk+aUUEfQFDDbMoQNNh+/vnn3bt3A1E6yLfR5sSJEzdu3AALRTNEdMmSJUCUFEOywQAF2sRgZZw6dcoa0sS0b9/+f//7H1gEaHxWqVIFTIihzjXly5fPyMgAK6NDhw4VKlQAS6dFixaNGzfGhSdPnlSrJu6Unw8fPrSxsQETQr6NNp06dfL09AQrgLvVtm/ffv78eRAzjx494rIsmAzybbTZsmVLREQEWA0zZ84U+68cEhJSo0YNMCHUbqNNUFBQdHQ0WBMffPABvm7atAnEibBqG+tstxk0aJCJ/UuBULt27blz54LYePbsma2trYn73dJ4G22sdlx+06ZN3d3dQd3gK6IObKa30IB8m8L8+++/Dx48AKskMDAQX3/44QeMTYFIQAtNWLKxTt/m8uXLGJMFK+aLL774448/QCSgbDi1mxLybbTp3bs3Wvlg3SxatAjULb8geMxipNF4G0Ivp0+fRotj+vTpIGCaN2/+33//gWkR69ydxuPw4cNeXl5cC7qV0759e4VC0HPZP3782PQWGpBvU5jbt29bbUigMJ06dQJ1kECYY3XQQjNxiw2H+OfuLGu6d+8uiqyCpmTcuHEDBw7EGCMIDNM3dHKQb0PwAOvhWrVqgWCYMmXKgAEDTD91F7XbaHPmzBmxd200HmgUCaoPjrlqG/JttMGWvlu3bgGhi169eglnUp3U1NSMjAyzDF8n30abN954Qy6XA6EH9HPwdefOnWYfVGKWFhsO6pOWR9euXRMSErhlhslz+dzd3WkCDJ20atUK24ULBgk6dOiAnkbfvn3BVJilWw0H+TZ5tGvXDqUiUYOy4VJyWvmMvwbw9fVds2YNqCcSxdcePXqgyYRVEJgQs3Sr4SDfJo/hw4cHBAQUXIONnhh1BUIPeH3w9dixY926dXv+/Dk+a2JiYkzZYG9GI436pOVRtWrVtm3bFpzgFn+Spk2bAmGQrVu3JiYmcstY8+zbtw9MhUBlY225BAYNGoTi4Zbd3NywQQCIoggNDdUs40Pn6tWrppmX6unTp+XLlzdXjiHybV5SsWLFjh07chVOlSpVMKQGhEEK18bx8fGmqXDM1a2Gg3ybVxg8eDAKxtHREWseIIpi1KhRDRs29PHxcXV1VarJzc09fPgwGB8zWmhguHMNyiYiIsKUdtrFA0l3LibnZCpzFUrQ87nw8xZwQHSU45fSV8YAGOhKxKiPDSWFBZYxuDurBKaoKXOkMkYmlZT3s3t3si8Imyc3Mk/vjs/MUChzWSWfLlpFXijdO5Xipyk+6uvPePra9fu4koHNBNQn7cK+pOCLL/zruNdr5SpDk1XTY51R38+aZdVl55ZfFYF6Mxajx0pWZ5FqAcPKBhpw8Z5Wgt59If+He/WCsepWnpe7a+0CBT6qgSPnI5VKn9xPu3c+KZdlR30l3Ewg4fczDq2Pq1Tdqd7r7q6utjrGF0gYUBa6tZj8K8jqWl/4VsQrBOrLq3WtCt4GOk6hLtJ1efWeKB+8/mEP0u9dTMxMz/3gW71ZFw3JxpTjbY5tehYanD74M38g1JzakhAflTpmgT8Ij+DzaUG744d+GQCWy8nNzxPj00bO9ddZKhTf5uGN1H6T/YHIp8MgT4mUObguHoTH+X3PGre38Hy/HYeUZ1m9118QfdJO70iwtZfYOgNREO+qTrFhaSAwQm9nKpVs3bYuYOlUrOYU/Vj39RdEn7QXz3OlMnHPL2sM3MrbRj4UXFfahNhshjGFd252nDxs5Pd1X39BtNvIs+Q5WblAvIpcniPPFtwgQnlOrjzbKvrFK+Ryfdef2m0Igjc03obgh9pAs/aB9DTeRtAI0IlQN1hYhW9joHmc+qQJGsqPYk4Y0Bf8EIRvw0iE+FgldMJYSRwNVA8tfY8tgfg2LGv15rJooJ9KIL4Nq2RUvfsIUcCwVhMSIN+GKCOsKSSgt9s1tdsIGKqBhQq12wgYhpwIs6IahaK7RBC+DSOxovCM2JHIGIl19B9kJUpWyL4Ny290oNUgyIeJaiyndZgg6tiHkH0blqH+GjpgaTqIotmxc0vnri3AtFCetLJk1+5tCxfNBcKE1K1T//1hY7llk11/6pNWljx4cBcI01KnTn3845bL9vozoNeLE8TcnfjhJDyt+KSkxIXffXXn7q0qlf379HkvKiribNDJP9dtx6Lc3Nw/1v5y8VJQfHxs/fqN+vYZ0KpVW26vd97tMmrkhOTkF39uWOXg4NC8WevJk2Z4epbHosTEhF9+XR5852ZWVlbz5q2HDxtbuXJVUKXPezTmg0ELv/1+6fJv3N3LrVn195Mnj//du/3a9f9iY6P9qwa8+eY7fXr3xy0/mTbu5s1ruHDkyP7ff/urZo3ad+7cwhPdv3/Hzb1c61ZvjBg+zsnJCUQOIwW+IYG5X38qlUq9vStu2bph3teL273RSeeV+Xfvjp9/WbZ/7xmZTHVbLl/xv737dq5ds7VaNVWiZyz99bcVe/ec6vded/x1zgSduHXr+p7dJ44ePYA/3PGjl8v8+rP6rWRB+DboYvKNCSxeOj8iMmzJ4l++WbD80qVz+CfJ/zF//Gnx9h2b+74zcPOmve3bdZ4779PTZ45zRTY2Nlu3bsAtd+86/ue6HbeDb6z/83dcr1Aopk4ff+Pm1amffIG/Uzl3j4mTRjyNjuJ2wdcNf60ZOOD96dNm4zL+tP/9d2HKx7O+W/gjauaHHxddvHQO13+/fBU+9rp1e+vk8Sv4m0U9jZzx6cSs7KyVP61bMG9paGjI1GnjUNIgdpQMX48Lr2Hok0f49+2C5a81aKzvyjRt2jInJyck5D63F/463t4++GTk3uITrVnTVqgoPNq+A7uqV6+1ZPHPjg6OmrMY4/qzJQgJmMy3kUj5tTsnpyRfvBg04L330a7FugLvZnzwc0XZ2dmHj+wbMnhk77f7ubm6vdmzT+dOPTZsXK3Zt1KlysOGjnZxdsEdsbZ5+PAeqKa5vREREfbF5wtatmjj4eH54YRPXN3cd+zYDJDXB7Z5s1bv9R9ap7aq4p0zZ+GSJb80ady8caNmWM/Uqlnn8n86Zl87duygjcwGf7AqVfz9/QNmTJ8T8uhB0LlTUGwYdc4iEBgs/0AFXkP8gebNXdymTTussfVdmUq+fhqdoDURHv6kW9e3bt2+zh0k+PaNJk1acEdzdXX7aNKMZk1bcvWSTkp//Q0giBzQSgW/QFpkRBi+1q/fkHvr7OzMXVBQzYV2D59YqAfNxo0aNkVDC5XGva1Zs46myMXFNT1dlWMBH2z4DEMlcOvxh8G9bt66ptmyZo2Xe+GNs3PnluEj+3Xs3Az/7j+4+yIpsfCHvHPnZu3a9dzc3Lm3Pj4VfX39NDdBcWBLl+5QUFStUk2Tr9nAlWnapGVw8E1cwLc1qtdq3Lj53TsqFT17Fh8TG4064XapVbPop3npr78BBOHb8CVNfa87Ob1MdYOPn7yitFR8/WjKGK1dkhIT3NTb6GwKwb3kcjlqoOBKfC5qlm3z545WKpWffTFFLs/5YOzkRo2aYa1V+FyaY6KitI6JHwOsEtsCk28buDKok59WLsGFmzevNmjQuG6dBrFxMagZtJ+9vLw5b1N1NFvbIs9Y+uuvmulIqrvIkGzQtwkODjZNSIBXwx43Abo8J0ezJulF3vPes7wqf9f0aV+iMVZwFy8vHwMHRIMNIwTffrOi4EqpRMc1exhyH13MpUt+aZpfv+HPU6G8V+EtPTzLN2jQCCMQBVe6ubpDsTFR/laeSBhWIi2V6WjgymAwJiUlGSsWrBaGv/8B/tC1atVFWyA4+EaTxvwaZ0p//fERySp0FwmiTxqehJe9XNFHlZ/3SdhjtFlBdeOmXbt2GQM1uOxXqQonKnQ8uI3RSsaDOzo6GjhgYGDNzMxMlBaa19ya6Jin7m7lCm+JUTh81egkLCwU/6r565jTKzCgxpGj+xu+1kQTq8At/fx45KcVZhswCxJWUSo5G7gyaBFUD6yPABAqAAAPzklEQVR5/tzpx49DcANc06B+o9u3r1+9dllLAKU5S+kRxvw2PPssop1atWo1jC1isAs18/0PCytWzEt0jfIYOWI8xgDQy0cnB2NoGE75/ofvDB8Qq44WLdosXbogLi4WhbF7zz8TPnz/0KF/C2+JEWd0Q7du25iSmoJRBLQoMFqAhgRXilXcvXvBGJtGrfbvPxQfOit/WYYR7cjI8N9X/Th67ECMJoHIKX3XBcNXBu20nbu24AORc0vq12uIYdKnTyM1jo0BTHb9hTHehn8P+U9nfIVPkfeH98WoInr5eHExbMIVDRo4fOaMrzZvWf92nw4YHfat6Dd9+uwiD4gtM+3bd5n/zefYtoM/W5cuPd99V8dcHRjq+fKLb+7eu93nnU5fzJ46dsyk3r374081YpSq6ebtt95Fa3Pmp5Meh4a4urj+sWarg73D+A+HYfwArfOZM+ZgYBSsHsNXBgMzWNVjnJp7i4YW2mwYHtA49wYw2fU3lDp99+7d6NvMnl30PVdKtv8QlRCbM+QzHqm4sU7ApwjexNzbz7/8RCaVLZi/FCyIq8efBwclT15unlld9XF+//Nrx5NHzBXWpzIGV08k3D33YuJSHd9UrONt5s3/DJsCPvxwKj6WsP346tVLWg49YSQkjITh2UtApDD6m6gEMt4G+DJ37qIlS+evXrPy2bM4bBOYO+c79DHA0hBiow2L8SXr6JjN6u8lIIh2G5Z/lYYhl2/mLwMLR4jjbdSDPKx9TCHlEhAyNHpPoAjCt2EkgnyuErrApnMr8W0MGMnCGG/DUtpW0aBybawlL4vee1IguQRoSLQuBJlLgBFo7wUjwOitb8i3ETCCzCXACrSvnBHQn35UEL6NhHwb8WBF7TYlGxRtMt9GqWTJtxELSlZpJb4NC3pTWwkkBzTlnyTEBPk2BMEbQfg2MhmDf0C8ilQqk9kI7rJIJVIBfipjYOD6C8K3cXKzS4inCda1yUpT2tgIzvt2dre1krnvcrKUMpnu6y8I36Z51/I5mSQbbeIj0t0q2IDAqNfaCT3l2CeZYOnEPE5399J9/QXh27h7g5un3c4fo4DIJzkJUl/k9p9SCYRHlRpOp7fFgUWT9gL/9F5/Q8PUUDYREREmGhcN8M/3T3Mz2W4j/Wwdwco5vychNPjFyK8CHZxBmJzZ8Tz0dka7/j4VKhedREZ0XNyb+OjWizHfBOjLkMMIqiH6n+VPE2KyGRko5Lo7PmFDm+71jKq3sIR5NWFkwV4gnDXOqjP2aX1j9WaMhFVNIZp/NB3HkQIoCh224MYSlmFffoCCB8lblrCgfNk35eUGBQ4oxd9Jwdg6SoZ96W8r7Bty35rYqJAM7lvg76VapfkiOq82k5+Mh1W3OBQefZC/V947reuj9WsWKNL6vfLeMuqzKAt1Bnr1LFqlUlsGFKyNg3TQdH8nN9CHIdmYK0/ajTMpWam5OoN4EqlEqdCxPj99pc7rl7fMqHurMEyhr5wnGwbb8fBdXFxceER4i+YttDaQSiDvzNqyyT+gRCUcTV9/1HCBZVR1/pb5n0qzY8GPZGsvCahfzqOiaHzuexfSkhOzlUot2eR9x4IX4eXPwd3Jhe87Bgr+gi8vi3pHJu+ZyL5yJPU2WrLhTsqFLVhWx0OQKdAHUqvUxk4S2KDo6y+IPGlaNGrnCubjxIlb0Q9PtXn7TSCKQZ3WaEcK1ZQ0GjR3pza5ubkGMgsTBND8NoVB2XCzDBCEPgTSJ01AUG1DFAn1SdOGZEMUCfk22pBsiCIh30YbuVxOsiEMQ76NNlTbEEVCvo02JBuiSMi30YZkQxQJ+TbaoGwMzyFFEOTbaIMhAWruJAxDvo02ZKQRRUK+jTYkG6JIyLfRhmRDFAn5NtqQb0MUCfk22lBtQxQJ+TbakGyIIiHfRhuSDVEk5NtoQ7IhioR8G21odCdRJOTbaIOykUqlQBD6Id9GGzLSiCIxZKQ9ePAgIiICrAxfX19UDhCEfgzJplatWmvXrt23bx9YDT///HNAQEDTpk2BIPRTdDLb5ORke3t7Ozs7sHQ2btyYmJg4ZcoUIAiDFD19ipub28WLFyMjI8Gi2bNnT1hYGGmGKA7FmnWoffv2P/zww4ULF8BCOXnyZFBQ0Jw5c4AgioGwZhwwC1evXl21atXvv/8OBFE8+M1xh9Z/VJRFTd4UEhKydOlS0gzBC961zaxZs8aPH4/hJhA/cXFxo0eP3r9/PxAEH6zXSMvMzOzWrdvZs2eBIHhSwomI58+fn5CQAGKmU6dOJ06cAILgTwll89VXX61YsSIlJQXESffu3bEZl7psEiXDGo209957b/HixdWqVQOCKBElrG00DB06NCcnB8TDmDFjZs+eTZohSkNpZbNp06Zly5aBSJg6derIkSMbNmwIBFEKrMhImzt3bsuWLd98k+ayJUpLaWsbjoyMjK5du4KAwSqxdu3apBmiTCgb2Tg6OmKj4bZt20CQrF692tnZefDgwUAQZUFZGmlKpTIxMbF8+fIgJLZu3RoRETFz5kwgiDKibGqbvGNJJNnZ2X369NGs6dWrF7rgYFqwTUazfPDgweDgYNIMUbaUpWyQSpUqbd68+eLFi7iM+omNjY2Pjw8JCQFTsWHDBqzxmjRpgsvnzp07dOjQggULgCDKlLLPNeHk5IQRXnS+UTD49tmzZ5cuXapRowaYhDNnzigUCqz3mjVrZmNjY8FjhAgzUsa1DceAAQM4zSB4E+NTH0xCdHR0XFwcaoZ7K5fLe/fuDQRR1pS9bNA2i4mJeXkCiSQqKso0Y6pv376t1cEUhURBZ6LMKXvZYDyNYZiCeQmxBrh8+TIYn6CgIIxJFPwkbm5uXl5eQBBlStn7Nnv37sUGnCNHjnAmE6sG7bR+/fqBMUGT7NatW5xi7e3tvb29W7dujVE16kpDlDmlare5cyE15Hrq85icXLlSkYvVCwPqF2BVryyrBJZhQX18lkVrTbWaYdRvNefPX365gHsweWvyt2QkwCoL7fLqNrib+rvgzoz6NIzmc0pkEjyshAGZncTHz75Be/cqNe2BIEpKSWSjyIHtP0clROfgnjJbqY1MautiI7PBWxtNPgXesqA+pur+VUmAUZ1D/YpFrFpJkKesvFL1B8nfS72kXgN52+UfKv8j522ptQ0nJ/albAsgxbPIcjJysjKyc7NzFXIl6rByDcfe432BIPjDWzZblkRi9WLnZONVzd2tohOIk/jHyUlRyfIcRZVaJB6CNzxkE/Uge8+ap3b2suptKoFFkJmUE3YzRiZjPviWht8QPCiubP47knTpcIJfXW93X0ewLKLvJiU+ffH+7GpuHjQ/B1EsiiWb+1fSj/8dW6+LP1goudmK+2cjR3zp70LKIYpB0bK5sC/p5tmk2h2qgqUTfCzs/c+quVUwSs8JwpIo4hZJe6G8eiLBGjSD+DXw+mtRKBBEURQhmw3fPClfxQ2sA3dvRwdn+z8XWN1MWARfDMlm3+oYRsL41PIAqyGgZcX0ZPm9S2lAEPoxJJuwe+mV6lQAK8OlgsvZPc+BIPSjVzbH/n4mtZG4+gg03Hzj9rEZc1qmpSdBWVP5Nc+crNyIB1lAEHrQK5vQW6lO7pbWRFNMbBxkZ3fGA0HoQa9ssrMUPrU8wSpx83J+8VxMqUYJE6N74MDNsykShrF1MFYLRljErSMn10RG3XV2KlenVttuHcfa26u6t527+M/R02s/HP3rhi2fx8WHVvSu3q7N4OZNenF77Tv005WbB+xsHRu/1t2rfBUwGj6B5Z6Flb35R1gMuoURFZIhtTVWe/nzhMjf138kl2dPHrdmxJBFMXEhv679UKHIxSKpzCYzM3X3/qUD3vliyfyLr9XvtG33N0kvYrHo/OUd5y9vf/etmVPGr/Ms53v05B9gPKSqQal3KZ5G6EG3bFIScxmJsaqaazcPyaQ2Iwcv8q7g7+MV8F6fL5/GPAi+d5orVSjkXTuOrVq5AcMwzRq9xbLs05iHuD7owrbX6nVGITk6umL9Uz2gGRgTRgLxkRQVIHSjWxsKhVLCgJFAC62yX10nJ3furUe5ip4efk/Cb2g2qFKpHrfg6OCKr5lZqSie54mR3l4v+yn7+dYGYyKRMNkZuUAQutDt20glDMsYSzeZWWmRT+9i+LjgypTUl6kzmEKnzspOVyoVdnYvI3u2tg5gTFi8CFKjPTkIkaNbNrZ2EmCN9ax1cfGsVrVR907jCq50cjLUhcfezkkikcrlL62m7JwMMDJObjTXGqEb3bJx97Z9FmOsCKyvd42rNw8E+DfWJDSLjQ+t4GkoMob1Tzn3imERt9u/nrfm3gPj5l5jFWylQONWaIR40e3b1GrsqpArwThgTFmpVP57cEVOTlb8s/B9h1cuWzkkJu6R4b0a1u9y++7JG7eP4fKJsxvCo4LBaGS9UNW0VeqQbAjd6JaNX007tOtT4oxiCGEobMbkzbY2Dt//NmLxjwNCw669986XRbr4XdqPatm0z+4Dy9Apwqqmd89PQJUPxyhzWj2LeGFjT6NuCL3oHaa2eVFkZpYksIUPWB/3T0VUrevYc4Q3EIQu9D5Tm3fzzErLBusjNwObXpWkGcIAerNy1mjsePIfiLz9vHID3dM8vUiOW7pyiM4iBzvnzGzdTew+FQImj1sNZcfsbzvrK8LbXyrV8QX9KzcYO/x7fXuF3Yx197IFgtCPoVwCj29nHNoQU6+Tv85SvCmTU3R3E0Zf39ZWd9pLiUTm7laWSZkTk6L1FeXIs21t7Aqvl0ltXV11PwsUmYr75yInLQsEgtCPoRzQgQ0cPbztHl98GthKR2I0fJB7lDN/Yr6y/Qwhl5/WauoCBGGQIuJFg2f45ebkxj22iu7AYVfiHJ2lXYbQDAVEERQdZh2/MOB5WHJ8aCpYNKGXYuTZ2cNnG3E8AmExFDcr588zHnv4ulasY5npOEIvx9jYKN//gjRDFAseOaB///wJI5XUfN0PLAhFDjw8H25nLxk9zx8Ionjwm3Fgy7LIhJhs53JOVZtYggPw6EJMVlpWYEPXniPInyF4wHuijtjQ7AN/xmSmKWwdZG7eLl7VRZZ8UCFXxoUkpcSn58oV7uXthn1RGQiCJyWcTS0uPOf0jviE2GylMm+aJomEwf9YZd7RCs7dBK/OgKZZA4VXvjLjk2oeKW4B8iePKrwlNwOU9um03kpAwjDY9q9UKPH72tnLKlSy6zuZprUhSghT2t6Q2XA9KDk2MjMjVaFglUp53mqJBDST3ubNO6jRg3pGQa2Vmr1YNm8lI2E4EeL2DIDy1Q7ZUim2t+YdXC3avA24qdsKnh2R2TC2dlJHF5l3FdsGba0lNy9hPBgjdSImCAum7GeKJgiLh2RDELwh2RAEb0g2BMEbkg1B8IZkQxC8+T8AAAD//5Rq3p0AAAAGSURBVAMAya6qX8H9oo4AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the agent workflow\n",
    "from langgraph.graph import StateGraph, START, END \n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "#Define the nodes \n",
    "workflow.add_node(\"agent\",agent) #agent \n",
    "retrieve = ToolNode([retriever_tool, s_retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve) #retriever tools\n",
    "workflow.add_node(\"rewrite\", rewrite) #rewrite\n",
    "workflow.add_node(\"generate\", generate) #grade documents\n",
    "\n",
    "#Define the edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    #Assess Agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\":\"retrieve\",\n",
    "        END:END\n",
    "    },\n",
    " )\n",
    "\n",
    "#Edges taken after the action node is called\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\",END)\n",
    "workflow.add_edge(\"rewrite\",\"agent\")\n",
    "\n",
    "#comppile \n",
    "\n",
    "graph = workflow.compile()\n",
    "from IPython.display import display, Image \n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37d12268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELAVANCE---\n",
      "--- Documents are relevant----\n",
      "--- GENERATE RESPONSE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Explain the data loading process in Snowflake.', additional_kwargs={}, response_metadata={}, id='05d83074-38b9-4b0c-8940-8c385a1182ff'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking about the data loading process in Snowflake. Let me think about what I need to do here. First, I should check if there\\'s a specific function provided that can help retrieve information related to Snowflake. Looking at the tools provided, there\\'s one called \"retriever_vector_s_db_blog\" which is for searching information about snowflake. Perfect, that\\'s exactly what I need here.\\n\\nThe user\\'s query is about explaining the data loading process, so I need to make sure I use the right parameters. The function requires a \"query\" parameter, which in this case would be \"data loading process\". I should structure the tool call with that query. Let me double-check the function\\'s parameters to ensure I\\'m formatting it correctly. The parameters are a JSON object with a \"query\" string. So the arguments should be {\"query\": \"data loading process in Snowflake\"}.\\n\\nWait, should I be more specific with the query? Maybe include \"steps\" or \"methods\"? But the user just wants an explanation of the process, so the key terms are \"data loading process\" and \"Snowflake\". I think that\\'s sufficient. I don\\'t want to overcomplicate the query. Let me go ahead and use the function as instructed. Make sure the JSON is correctly formatted and enclosed within the tool_call tags. Alright, that should do it.\\n', 'tool_calls': [{'id': 'px24ps9f3', 'function': {'arguments': '{\"query\":\"data loading process in Snowflake\"}', 'name': 'retriever_vector_s_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 225, 'total_tokens': 541, 'completion_time': 0.593767074, 'completion_tokens_details': {'reasoning_tokens': 282}, 'prompt_time': 0.009030151, 'prompt_tokens_details': None, 'queue_time': 0.053766296, 'total_time': 0.602797225}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2344-a791-7fd1-b011-ca998bda8cdf-0', tool_calls=[{'name': 'retriever_vector_s_db_blog', 'args': {'query': 'data loading process in Snowflake'}, 'id': 'px24ps9f3', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 225, 'output_tokens': 316, 'total_tokens': 541, 'output_token_details': {'reasoning': 282}}),\n",
       "  ToolMessage(content=\"Welcome to Snowflake Documentation | Snowflake DocumentationDOCUMENTATION/Get startedGuidesDeveloperReferenceRelease notesTutorialsStatusOverviewSnowflake Horizon CatalogApplications and tools for connecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData typesData IntegrationSnowflake OpenflowApache Icebergâ„¢Apache Icebergâ„¢ TablesSnowflake Open CatalogData engineeringData loadingDynamic TablesStreams and Tasksdbt Projects on SnowflakeData UnloadingStorage Lifecycle PoliciesMigrationsQueriesListingsCollaborationSnowflake AI & MLSnowflake PostgresAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness continuity & data recoveryPerformance optimizationCost & Billing\\n\\nWelcome to Snowflake DocumentationÂ¶\\n\\n\\n\\nWELCOME TO SNOWFLAKEÂ¶\\n\\nDOCUMENTATION\\n\\n\\n\\nIn these topics, you will find the information you need to access your Snowflake account and perform all the administrative and user tasks associated\\nwith using Snowflake. The documentation also provides conceptual overviews, tutorials, and a detailed reference for all supported SQL commands,\\nfunctions, and operators.\\nYou can start by browsing the contents on the left or using the search box at the top to search across the documentation and other Snowflake resources.\\nIf you do not find the information you are looking for, please feel free to reach out to Snowflake Documentation or Snowflake Support using the buttons\\nat the bottom of each page.\\n\\n\\n\\nGet started with Snowflake for usersÂ¶\\n\\nBefore you beginOverview of getting an account and methods for accessing Snowflake.\\n\\nSign in to SnowflakeOverview of the different ways to connect to Snowflake.\\n\\nSnowflake key concepts and architectureDescription of Snowflake architecture, key concepts, and features.\\n\\nSnowsight quick tourOverview of Snowsight, Snowflakeâ€™s web-based interface.\\n\\nOverview of the data lifecycleIntroduces the main operations and corresponding SQL commands for getting your data into Snowflake and\\nthen using it to perform queries and other SQL operations.\\n\\n\\n\\n\\nTutorials and Other ResourcesÂ¶\\nThis topic provides links to assorted â€œhow toâ€ tutorials/labs and â€œbest practicesâ€ for using Snowflake.\\n\\n\\nUsing SnowflakeÂ¶\\n\\nSnowsight: The Snowflake web interface â€” Learn how to use Snowsight for your Snowflake operations:\\n\\n\\nSnowsight quick tour\\nGetting started with Snowsight\\nWork with worksheets in Snowsight\\nWorkspaces\\nAbout Snowflake Notebooks\\nUsing Snowflake Copilot\\nVisualizing data with dashboards\\nExplore and manage database objects in Snowsight\\nMonitor query activity with Query History\\nEvaluating and monitoring account security in the Trust Center\\nManage Snowflake Support cases\\nSet up and manage notification contacts for Snowflake\\n\\n\\n\\nVirtual warehouses â€” Key concepts and tasks for creating and using virtual warehouses to execute queries and perform DML operations, such as loading and unloading data:\\n\\n\\nOverview of warehouses\\nMulti-cluster warehouses\\nWarehouse considerations\\nWorking with warehouses\\nUsing the Query Acceleration Service (QAS)\\nMonitoring warehouse load\\n\\n\\n\\nDatabases, Tables & Views â€” Key concepts and tasks related to understanding and working with Snowflake databases and tables:\\n\\n\\nUnderstanding Snowflake Table Structures\\nWorking with Temporary and Transient Tables\\nIntroduction to external tables\\nOverview of Views\\nWorking with Secure Views\\nWorking with Materialized Views\\nTable Design Considerations\\nCloning considerations\\nData storage considerations\\n\\n\\n\\nQuery Data in Snowflake â€” Key concepts and tasks for executing queries in Snowflake:\\n\\n\\nWorking with joins\\nUnderstanding How Snowflake Can Eliminate Redundant Joins\\nWorking with Subqueries\\nQuerying Hierarchical Data\\nWorking with CTEs (Common Table Expressions)\\nQuerying Semi-structured Data\\nAnalyzing data with window functions\\nIdentifying Sequences of Rows That Match a Pattern\\nUsing Sequences\\nUsing Persisted Query Results\\nComputing the Number of Distinct Values\\nEstimating Similarity of Two or More Sets\\nEstimating Frequent Values\\nEstimating Percentile Values\\nQuerying data using worksheets\\nCanceling Statements\\n\\n\\n\\nIntroduction to Loading Semi-structured Data â€” Key concepts and tasks for working with JSON and other types of semi-structured data:\\n\\n\\nSupported formats for semi-structured data\\nConsiderations for semi-structured data stored in VARIANT\\nTutorial: JSON basics for Snowflake\\n\\n\\n\\nIntroduction to unstructured data â€” Key concepts and tasks for working with unstructured data:\\n\\n\\nDirectory tables\\nREST API for unstructured data support\\nSharing unstructured data with a secure view\\nTroubleshooting processing of unstructured data\\n\\n\\n\\nSnowflake Time Travel & Fail-safe â€” Key concepts and tasks for understanding how Snowflake maintains access to deleted and modified data, and also how Snowflake enables data recovery in the\\nevent of loss:\\n\\n\\nUnderstanding & using Time Travel\\nUnderstanding and viewing Fail-safe\\nStorage costs for Time Travel and Fail-safe\\n\\n\\n\\nIntroduction to Streams and Tasks â€” Key concepts and tasks for transforming and optimizing loaded data for analysis:\\n\\n\\nIntroduction to Streams\\nIntroduction to tasks\\n\\n\\n\\nIntroduction to business continuity & disaster recovery â€” Key concepts and tasks for replicating and failing over databases across multiple Snowflake accounts, as well as redirecting client connections, for business continuity and disaster recovery:\\n\\n\\nIntroduction to replication and failover across multiple accounts\\nRedirecting client connections\\n\\n\\n\\nSample data sets â€” Key concepts and tasks for using the sample data sets provided with Snowflake:\\n\\n\\nUse the sample database\\nSample data: TPC-H\\nSample Data: OpenWeatherMap â€” Deprecated\\n\\n\\n\\nAlerts and Notifications â€” Key concepts and tasks for sending email notifications in SQL (e.g. from a\\nstored procedure, task, etc.) and setting up alerts to perform actions or send notifications when data in Snowflake meets\\ncertain conditions.\\n\\n\\nSetting up alerts based on data in Snowflake\\nNotifications in Snowflake\\n\\n\\n\\nSnowflake Postgres â€” Create, manage, and use Postgres instances directly from Snowflake:\\n\\n\\nCreating a Snowflake Postgres Instance\\nConnecting to Snowflake Postgres\\nSnowflake Postgres Roles\\nSnowflake Postgres Connection Pooling\\nSnowflake Postgres Maintenance\\nSnowflake Postgres Read Replicas\\nSnowflake Postgres High Availability\\nSnowflake Postgres Cost Evaluation\\nSnowflake Postgres Insights\\nSnowflake Postgres logging\\nSnowflake Postgres networking\\nSnowflake Postgres Instance Sizes\\nSnowflake Postgres Extensions\\nSnowflake Postgres Server Settings\\n\\n\\n\\n\\n\\n\\nManaging Your Snowflake AccountÂ¶\\n\\nAccount identifiers\\n\\nDetailed descriptions of the two unique account identifiers supported for connecting to Snowflake and using features that span multiple accounts.\\n\\n\\nTrial accounts\\n\\nInstructions for signing up for a trial account, adding a credit card to the account, and canceling the account.\\n\\n\\nParameter management\\n\\nInstructions for setting account, session, and object parameters for your account.\\n\\n\\nUser management\\n\\nInstructions for creating and managing users in your account.\\n\\n\\nBehavior change management\\n\\nInstructions for enabling and disabling behavior change releases in your account.\\n\\n\\n\\n\\n\\nGeneral referenceÂ¶\\n\\nParameters â€” parameters that can be used to control system behavior at the account, user, session, and object\\nlevel.\\nReferences â€” use references to authorize access on objects for ownerâ€™s rights stored procedures,\\napplications, and classes.\\nTernary logic â€” information about the behavior of NULL in Boolean expressions and with comparison operators.\\nCollation support â€” information about sorting and other character-set-dependent operations on text strings.\\nSQL format models â€” formats for specifying conversion of numeric and date/time values to and from text strings.\\nObject identifiers â€” rules for defining and using object identifiers, including resolving object names used in SQL\\nstatements:\\n\\nIdentifier requirements\\nLiterals and variables as identifiers with IDENTIFIER() syntax\\nObject name resolution\\n\\n\\nConstraints â€” concepts and reference information for defining and maintaining unique, primary key, and foreign\\nkey constraints in tables:\\n\\nOverview of Constraints\\nCreating Constraints\\nModifying Constraints\\nDropping Constraints\\n\\n\\nSQL variables â€” concepts and reference for defining and using variables in sessions.\\nTransactions â€” concepts and reference for using transactions with SQL statements.\\nTable literals â€” concepts and reference for using table literals instead of a single scalar value in queries.\\nSNOWFLAKE database â€” reference for the SNOWFLAKE shared database, which is provided by Snowflake for\\nquerying/reporting on your organization, account, data sharing, and other object usage.\\nSnowflake Information Schema â€” concepts and reference for the Snowflake Information Schema, which consists of a set of metadata\\nviews and historical table functions for querying/reporting on objects in Snowflake.\\nMetadata fields in Snowflake â€” concepts and reference for metadata fields in Snowflake.\\n\\n\\n\\nSQL command referenceÂ¶\\n\\nQuery syntax â€” structure of SQL queries in Snowflake.\\nQuery operators â€” arithmetic, logical, and other types of operators.\\nData Definition Language (DDL) commands â€” overview of DDL commands.\\nData Manipulation Language (DML) commands â€” commands for performing DML operations, including:\\n\\nInserting, deleting, updating, and merging data in Snowflake tables.\\nBulk copying data into and out of Snowflake tables.\\nStaging files for bulk copying.\\n\\n\\nAll commands (alphabetical) â€” alphabetical list of all the commands.\\nCommands categorized by the type of objects and operations they control, including:\\n\\nGeneral account-level objects (accounts, users, roles, security policies, integrations, etc.) and operations (failover & recovery, etc.).\\nSession-based operations (session context, queries, variables, transactions, etc.).\\nVirtual warehouses (for loading data and performing queries) and resource monitors (for controlling credit usage).\\nDatabases, schemas, tables, and other schema-level objects (views, sequences, etc.).\\nSnowflake extensions and application development (user-defined functions, stored procedures, scripting, etc.).\\nObjects for sharing data (shares, listings, etc.).\\nObjects for classifying, protecting, and governing data (masking policies, row-access policies, tags, etc.).\\n\\n\\n\\n\\n\\nFunction and stored procedure referenceÂ¶\\n\\nSummary of functions â€” combined summary of all system-defined functions. Can be used as a\\nquick-reference.\\nAll functions (alphabetical) â€” alphabetical list of all system-defined functions (scalar, aggregate, table, etc.).\\nAggregate functions â€” functions that take multiple rows/values as input and return a single value.\\nScalar functions â€” functions that take a single row/value as input and return a single value:\\n\\nBitwise expression functions\\nConditional expression functions\\nContext functions\\nConversion functions\\nData generation functions\\nDate & time functions\\nDifferential privacy functions\\nEncryption functions\\nGeospatial functions\\nHash functions\\nMetadata functions\\nNotification functions\\nNumeric functions\\nSemi-structured and structured data functions\\nString functions (regular expressions)  â€” regular expression (search) functions\\nString & binary functions\\nVector functions\\n\\n\\nModel monitor functions â€” functions that retrieve metrics from machine learning model monitors.\\nSystem functions â€” functions that perform control operations or return system-level information.\\nTable functions â€” functions that return results in tabular format.\\nWindow functions â€” functions that run analytic calculations, such as moving aggregations and rankings.\\nData metric functions â€” functions that enable data quality measurements for tables and views.\\nStored procedures â€” stored procedures to facilitate using certain Snowflake features.\\n\\n\\n\\nSnowflake Scripting referenceÂ¶\\n\\nAWAIT\\nBEGIN â€¦ END\\nBREAK\\nCANCEL\\nCASE\\nCLOSE\\nCONTINUE\\nDECLARE\\nEXCEPTION\\nFETCH\\nFOR\\nIF\\nLET\\nLOOP\\nNULL\\nOPEN\\nRAISE\\nREPEAT\\nRETURN\\nWHILE\\n\\n\\n\\nAppendicesÂ¶\\n\\nNotational conventions\\n\\nNotational conventions used in the Snowflake documentation.\\n\\n\\nReserved & limited keywords\\n\\nList of words reserved for Snowflake SQL.\\n\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite TermsCookies SettingsÂ© 2026 Snowflake, Inc. All Rights Reserved.On this pageGet started with Snowflake for usersTutorials and Other ResourcesUsing SnowflakeManaging Your Snowflake AccountGeneral referenceSQL command referenceFunction and stored procedure referenceSnowflake Scripting referenceAppendicesLanguage: EnglishEnglishFranÃ§aisDeutschæ—¥æœ¬èªží•œêµ­ì–´PortuguÃªs\\n\\nOverview of data loading | Snowflake DocumentationDOCUMENTATION/Get startedGuidesDeveloperReferenceRelease notesTutorialsStatusOverviewSnowflake Horizon CatalogApplications and tools for connecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData typesData IntegrationSnowflake OpenflowApache Icebergâ„¢Apache Icebergâ„¢ TablesSnowflake Open CatalogData engineeringData loadingOverviewFeature summaryConsiderationsPreparing to load dataStaging files using SnowsightLoading data using SnowsightMonitor data loading activityBulk LoadingLocal File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingMonitoring Events for SnowpipeManaging Snowpipe in SnowsightSnowpipe CostsSnowpipe StreamingOverviewHigh-performance ArchitectureClassic ArchitectureSemi-Structured DataIntroductionSupported FormatsConsiderationsUnstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingLoading Unstructured Data with Document AIAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyIntegrate Snowflake into external applicationsSnowflake Connector for Microsoft Power AppsAbout the connectorTasksLoading data from third-party systemsLoading data using Native ApplicationsDynamic TablesStreams and Tasksdbt Projects on SnowflakeData UnloadingStorage Lifecycle PoliciesMigrationsQueriesListingsCollaborationSnowflake AI & MLSnowflake PostgresAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness continuity & data recoveryPerformance optimizationCost & BillingGuidesData engineeringData loadingOverview\\n\\nOverview of data loadingÂ¶\\nThis topic provides an overview of the main options available to load data into Snowflake.\\n\\nSupported file locationsÂ¶\\nSnowflake refers to the location of data files in cloud storage as a stage.\\nThe COPY INTO <table> command used for both bulk and continuous data loads (Snowpipe) supports\\ncloud storage accounts managed by your business entity (external stages) as well as cloud storage contained in your Snowflake account\\n(internal stages).\\n\\nExternal stagesÂ¶\\nLoading data from any of the following cloud storage services is supported regardless of the cloud platform that hosts your Snowflake account:\\n\\nAmazon S3\\nGoogle Cloud Storage\\nMicrosoft Azure\\n\\nYou cannot access data held in archival cloud storage classes that requires restoration before it can be retrieved. These archival storage classes include, for example, the Amazon S3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, or Microsoft Azure Archive Storage.\\nUpload (i.e. stage) files to your cloud storage account using the tools provided by the cloud storage service.\\nA named external stage is a database object created in a schema. This object stores the URL to files in cloud storage, the settings used to access the cloud storage account, and convenience settings such as the options that describe the format of staged files. Create stages using the CREATE STAGE command.\\n\\nNote\\nSome data transfer billing charges may apply when loading data from files in a cloud storage service in a different region or cloud platform from your Snowflake account. For more information, see Understanding data transfer cost.\\n\\n\\n\\nInternal stagesÂ¶\\nSnowflake maintains the following stage types in your account:\\n\\nUser:\\nA user stage is allocated to each user for storing files. This stage type is designed to store files that are staged and managed by a single user but can be loaded into multiple tables. User stages cannot be altered or dropped.\\n\\nTable:\\nA table stage is available for each table created in Snowflake. This stage type is designed to store files that are staged and managed by one or more users but only loaded into a single table. Table stages cannot be altered or dropped.\\nNote that a table stage is not a separate database object; rather, it is an implicit stage tied to the table itself. A table stage has no grantable privileges of its own. To stage files to a table stage, list the files, query them on the stage, or drop them, you must be the table owner (have the role with the OWNERSHIP privilege on the table).\\n\\nNamed:\\nA named internal stage is a database object created in a schema. This stage type can store files that are staged and managed by one or more users and loaded into one or more tables. Because named stages are database objects, the ability to create, modify, use, or drop them can be controlled using security access control privileges. Create stages using the CREATE STAGE command.\\n\\n\\nUpload files to any of the internal stage types from your local file system using the PUT command.\\n\\n\\n\\nBulk vs continuous loadingÂ¶\\nSnowflake provides the following main solutions for data loading. The best solution may depend upon the volume of data to load and the frequency of loading.\\n\\nBulk loading using the COPY commandÂ¶\\nThis option enables loading batches of data from files already available in cloud storage, or copying (i.e. staging) data files from a local machine to an internal (i.e. Snowflake) cloud storage location before loading the data into tables using the COPY command.\\n\\nCompute resourcesÂ¶\\nBulk loading relies on user-provided virtual warehouses, which are specified in the COPY statement. Users are required to size the warehouse appropriately to accommodate expected loads.\\n\\n\\nSimple transformations during a loadÂ¶\\nSnowflake supports transforming data while loading it into a table using the COPY command. Options include:\\n\\nColumn reordering\\nColumn omission\\nCasts\\nTruncating text strings that exceed the target column length\\n\\nThere is no requirement for your data files to have the same number and ordering of columns as your target table.\\n\\n\\n\\nContinuous loading using SnowpipeÂ¶\\nThis option is designed to load small volumes of data (i.e. micro-batches) and incrementally make them available for analysis. Snowpipe loads data within minutes after files are added to a stage and submitted for ingestion. This ensures users have the latest results, as soon as the raw data is available.\\n\\nCompute resourcesÂ¶\\nSnowpipe uses compute resources provided by Snowflake (i.e. a serverless compute model). These Snowflake-provided resources are automatically resized and scaled up or down as required, and are charged and itemized using per-second billing. Data ingestion is charged based upon the actual workloads.\\n\\n\\nSimple transformations during a loadÂ¶\\nThe COPY statement in a pipe definition supports the same COPY transformation options as when bulk loading data.\\nIn addition, data pipelines can leverage Snowpipe to continuously load micro-batches of data into staging tables for transformation and optimization using automated tasks and the change data capture (CDC) information in streams.\\n\\n\\n\\nContinuous loading using Snowpipe StreamingÂ¶\\nThe Snowpipe Streaming API writes rows of data directly to Snowflake tables without the requirement of staging files. This architecture results in lower load latencies with corresponding lower costs for loading any volume of data, which makes it a powerful tool for handling near real-time data streams.\\nSnowpipe Streaming is also available for the Snowflake Connector for Kafka, which offers an easy upgrade path to take advantage of the lower latency and lower cost loads.\\nFor more information, refer to Snowpipe Streaming.\\n\\n\\n\\nLoading data from Apache Kafka topicsÂ¶\\nThe Snowflake Connector for Kafka enables users to connect to an Apache Kafka server, read data from one or more topics, and load that data into Snowflake tables.\\n\\n\\nSchema detection of column definitions from staged semi-structured data filesÂ¶\\nSemi-structured data can include thousands of columns. Snowflake provides robust solutions for handling this data. Options include\\nreferencing the data directly in cloud storage using external tables, loading the data into a single column of type VARIANT, or\\ntransforming and loading the data into separate columns in a standard relational table. All of these options require some knowledge of the\\ncolumn definitions in the data.\\nA different solution involves automatically detecting the schema in a set of staged semi-structured data files and retrieving the column\\ndefinitions. The column definitions include the names, data types, and ordering of columns in the files. Generate syntax in a format\\nsuitable for creating Snowflake standard tables, external tables, or views.\\n\\nNote\\nThis feature supports Apache Parquet, Apache Avro, ORC, JSON, and CSV files.\\n\\nThis support is implemented through the following SQL functions:\\n\\nINFER_SCHEMADetects the column definitions in a set of staged data files and retrieves the metadata in a format suitable for creating Snowflake objects.\\n\\nGENERATE_COLUMN_DESCRIPTIONGenerates a list of columns from a set of staged files using the INFER_SCHEMA function output.\\n\\n\\nThese SQL functions support both internal and external stages.\\nCreate tables or external tables with the column definitions derived from a set of staged files using the\\nCREATE TABLE â€¦ USING TEMPLATE or CREATE EXTERNAL TABLE â€¦ USING TEMPLATE syntax. The USING TEMPLATE clause accepts an expression that\\ncalls the INFER_SCHEMA SQL function to detect the column definitions in the files. After the table is created, you can then use a COPY statement with the MATCH_BY_COLUMN_NAME option to load files directly into the structured table.\\nSchema detection can also be used in conjunction with table schema evolution, where the structure of tables evolves automatically to support the structure of new data received from the data sources.\\n\\n\\nAlternatives to loading dataÂ¶\\nYou can use the following option to query your data in cloud storage without loading it into Snowflake tables.\\n\\nExternal tables (data lake)Â¶\\nExternal tables enable querying existing data stored in external cloud storage for analysis without first loading it into Snowflake. The source of truth for the data remains in the external cloud storage. Data sets materialized in Snowflake via materialized views are read-only.\\nThis solution is especially beneficial to accounts that have a large amount of data stored in external cloud storage and only want to query a portion of the data; for example, the most recent data. Users can create materialized views on subsets of this data for improved query performance.\\n\\n\\nWorking with Amazon S3-compatible storageÂ¶\\nYou can create external stages and tables in Snowflake to access storage in an application or device that is Amazon S3-compatible.\\nThis feature lets you manage, govern, and analyze your data, regardless of where the data is stored.\\nFor information, see Working with Amazon S3-compatible storage.\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite TermsCookies SettingsÂ© 2026 Snowflake, Inc. All Rights Reserved.On this pageSupported file locationsBulk vs continuous loadingLoading data from Apache Kafka topicsSchema detection of column definitions from staged semi-structured data filesAlternatives to loading dataRelated contentOverview of data unloadingData Manipulation Language (DML) commandsTable schema evolutionINFER_SCHEMALoad data into Apache Icebergâ„¢ tablesLanguage: EnglishEnglishFranÃ§aisDeutschæ—¥æœ¬èªží•œêµ­ì–´PortuguÃªs\\n\\nSet up and access Openflow | Snowflake DocumentationDOCUMENTATION/Get startedGuidesDeveloperReferenceRelease notesTutorialsStatusOverviewSnowflake Horizon CatalogApplications and tools for connecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData typesData IntegrationSnowflake OpenflowSet up and access OpenflowAbout Openflow - BYOC DeploymentsAbout Openflow - Snowflake DeploymentsConnect your data sources using Openflow connectorsAbout Openflow connectorsAbout SAPÂ® and SnowflakeOpenflow Connector for Amazon AdsAbout the connectorSet up the connectorOpenflow Connector for BoxAbout the connectorSet up the connectorOpenflow Connector for Google AdsAbout the connectorSet up the connectorOpenflow Connector for Google DriveAbout the connectorSet up the connectorOpenflow Connector for Google SheetsAbout the connectorSet up the connectorOpenflow Connector for Jira CloudAbout the connectorSet up the connectorOpenflow Connector for KafkaAbout the connectorSet up the connector (core)Performance tuningOpenflow Connector for Kinesis Data StreamsAbout the connectorSet up the connectorMaintain the connectorTroubleshoot the connectorOpenflow Connector for LinkedIn AdsAbout the connectorSet up the connectorOpenflow Connector for Meta AdsAbout the connectorSet up the connectorOpenflow Connector for Microsoft DataverseAbout the connectorSet up the connectorOpenflow Connector for MySQLAbout the connectorSet up the connectorSet up incremental replication without snapshotsMaintenanceOpenflow Connector for PostgreSQLAbout the connectorSet up the connectorSet up incremental replication without snapshotsMaintenanceOpenflow Connector for SharePointAbout the connectorSet up the connectorOpenflow Connector for SlackAbout the connectorSet up the connectorOpenflow Connector for Snowflake to KafkaAbout the connectorSet up the connectorOpenflow Connector for SQL ServerAbout the connectorSet up the connectorSet up incremental replication without snapshotsMaintenanceOpenflow Connector for WorkdayAbout the connectorSet up the connectorManage OpenflowMonitor OpenflowTroubleshoot OpenflowProcessorsController servicesVersion historyApache Icebergâ„¢Apache Icebergâ„¢ TablesSnowflake Open CatalogData engineeringData loadingDynamic TablesStreams and Tasksdbt Projects on SnowflakeData UnloadingStorage Lifecycle PoliciesMigrationsQueriesListingsCollaborationSnowflake AI & MLSnowflake PostgresAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness continuity & data recoveryPerformance optimizationCost & BillingGuidesData IntegrationSnowflake OpenflowSet up and access Openflow\\n\\nSet up and access OpenflowÂ¶\\n\\nFeature â€” Generally Available\\nOpenflow Snowflake Deployments are available to all accounts in AWS and Azure Commercial regions.\\nOpenflow BYOC deployments are available to all accounts in AWS Commercial regions.\\n\\nTo use Openflow, you must configure roles and permissions in your Snowflake account, and set up a database. This topic describes how to set up the necessary roles and permissions.\\n\\nSet up the Openflow admin rolesÂ¶\\nThe Openflow Admin role is used by a deployment engineer to set up Openflow workflows. A Snowflake administrator adds this role by performing the following steps:\\n\\nSign in to Snowsight.\\nOpen a SQL worksheet.\\nCreate a role for the Openflow admin, allowing it the required permissions to manage integrations and compute pools required for deployments. In the SQL below, OPENFLOW_ADMIN is the default name for the Openflow admin, but you can choose any name.\\nUSE ROLE ACCOUNTADMIN;\\n\\nCREATE ROLE IF NOT EXISTS OPENFLOW_ADMIN;\\n\\nGRANT CREATE ROLE ON ACCOUNT TO ROLE OPENFLOW_ADMIN;\\n\\nGRANT CREATE OPENFLOW DATA PLANE INTEGRATION ON ACCOUNT\\n   TO ROLE OPENFLOW_ADMIN;\\n\\nGRANT CREATE OPENFLOW RUNTIME INTEGRATION ON ACCOUNT\\n   TO ROLE OPENFLOW_ADMIN;\\n\\nCopy\\n\\nGrant the admin role and secondary roles to a user.\\nTo prevent issues with login, when you create an Openflow user, Snowflake recommends that you also assign and set default secondary roles to that user. This is helpful because Openflow doesnâ€™t allow users with the following roles to log in: ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, or SECURITYADMIN. While logged in, Openflow actions can be authorized by any of the authenticated userâ€™s roles, not just the default role.\\nSubstitute <OPENFLOW_USER> with the appropriate username:\\nUSE ROLE ACCOUNTADMIN;\\nGRANT ROLE OPENFLOW_ADMIN TO USER <OPENFLOW_USER>;\\nALTER USER <OPENFLOW_USER> SET DEFAULT_ROLE = OPENFLOW_ADMIN;\\nALTER USER <OPENFLOW_USER> SET DEFAULT_SECONDARY_ROLES = ('ALL');\\n\\nCopy\\n\\n\\n\\n\\nAccept the Openflow terms of serviceÂ¶\\nThis step is only required once for your organization.\\n\\nSign in to Snowflake as a user with the ORGADMIN role.\\nIn the navigation menu, select Ingestion Â» Openflow.\\nReview the agreement and select Accept.\\n\\n\\n\\nStart OpenflowÂ¶\\nLog in to Openflow by performing the following steps:\\n\\nSign in to Snowsight.\\nIn the navigation menu, select Ingestion Â» Openflow.\\nSelect Launch Openflow.\\n\\n\\nTroubleshooting login issuesÂ¶\\n\\nIf you can log into Snowflake but canâ€™t log into Openflow, try the following:\\n\\nTry changing your role to something other than ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, or SECURITYADMIN.\\nTry adding default secondary roles to the account:\\nUSE ROLE ACCOUNTADMIN;\\nALTER USER <OPENFLOW_USER> SET DEFAULT_SECONDARY_ROLES = ('ALL');\\n\\nCopy\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite TermsCookies SettingsÂ© 2026 Snowflake, Inc. All Rights Reserved.On this pageSet up the Openflow admin rolesAccept the Openflow terms of serviceStart OpenflowRelated contentSet up Openflow - BYOCSet up Openflow - Snowflake Deployment - Task overviewManage OpenflowOpenflow connectorsAll processors (alphabetical)All controller services (alphabetical)Language: EnglishEnglishFranÃ§aisDeutschæ—¥æœ¬èªží•œêµ­ì–´PortuguÃªs\", name='retriever_vector_s_db_blog', id='fa516881-3a92-420a-81dc-01ec9bbc7bd2', tool_call_id='px24ps9f3'),\n",
       "  AIMessage(content=\"<think>\\nOkay, I need to explain the data loading process in Snowflake based on the provided documentation. Let me start by going through the key sections mentioned in the docs.\\n\\nFirst, the overview of data loading mentions that Snowflake uses stages to store data files, which can be either internal (like user or table stages) or external (cloud storage). That makes sense because stages act as the location where data is stored before loading.\\n\\nThe main methods are bulk loading with COPY INTO and continuous loading with Snowpipe. Bulk loading uses virtual warehouses, while Snowpipe is more automated and serverless. The documentation also touches on semi-structured data and schema detection, which are important for handling complex data types. There's a mention of external tables as an alternative to loading data into Snowflake tables directly.\\n\\nI should outline the steps involved in the process. First, preparing by setting up stages, then staging the data files, followed by the actual loading using COPY INTO or Snowpipe. Also, the documentation mentions handling semi-structured data and schema evolution. I need to highlight these as part of the process.\\n\\nWait, the user also provided a section on Openflow setup. Should I include that? The user's original question was about the data loading process, so maybe focus on the core methods like stages, COPY INTO, Snowpipe, and external tables. Openflow seems to be a specific integration tool, but maybe not essential for the general explanation unless the user wants a comprehensive answer.\\n\\nThe troubleshooting and considerations are also important. For example, the note about archival storage classes in cloud storage not being accessible. Also, the difference between internal and external stages, and how they affect performance.\\n\\nI should structure the answer by first explaining the main components (stages), then the two primary methods (bulk vs continuous), mention handling semi-structured data, alternatives like external tables, and maybe touch on schema detection and transformation options during loading.\\n\\nMake sure to clarify the roles of virtual warehouses in bulk loading versus the serverless aspect of Snowpipe. Also, mention the supported cloud storage services (S3, GCS, Azure) and the need for proper permissions.\\n\\nI need to avoid getting too deep into specific connectors or advanced topics unless necessary. The user wants an explanation of the process, so it's better to keep it high-level but cover all the key steps and options available in Snowflake for loading data.\\n</think>\\n\\n### **Data Loading Process in Snowflake**  \\nSnowflake offers robust, scalable solutions for loading data into its cloud-native data platform. The process involves staging data in cloud storage or internal Snowflake stages, then using tools like `COPY INTO` or **Snowpipe** to load data into tables. Below is a structured explanation of the process:\\n\\n---\\n\\n#### **1. Staging Data**  \\nData is stored in **stages**â€”Snowflake's abstraction for cloud storage locations.  \\n- **External Stages**:  \\n  - Point to external cloud storage (Amazon S3, Google Cloud Storage, Microsoft Azure).  \\n  - Example:  \\n    ```sql\\n    CREATE STAGE my_s3_stage \\n      URL = 's3://my-bucket/data/' \\n      CREDENTIALS = (AWS_KEY_ID='...', AWS_SECRET_KEY='...');\\n    ```  \\n  - **Note**: Archival storage classes (e.g., S3 Glacier) are not supported unless data is restored.  \\n\\n- **Internal Stages**:  \\n  - **User Stage**: Per-user for ad-hoc file transfers.  \\n  - **Table Stage**: Tied to a specific table.  \\n  - **Named Internal Stage**: A schema-scoped database object for shared access.  \\n  - Example:  \\n    ```sql\\n    CREATE STAGE my_named_stage;\\n    PUT file:///local/data.csv @my_named_stage;  -- Upload from local file system.\\n    ```\\n\\n---\\n\\n#### **2. Bulk Data Loading (`COPY INTO`)**  \\nFor large batch loads:  \\n- **Steps**:  \\n  1. **Stage Files**: Use `PUT` command to upload files to internal stages or ensure files exist in external stages.  \\n  2. **Load Data**: Use `COPY INTO <table>` to load staged files into a table.  \\n     ```sql\\n     COPY INTO my_table\\n     FROM @my_stage\\n     FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',');\\n     ```  \\n  3. **Transformations During Load**:  \\n     - Reorder columns, cast types, truncate text, or omit columns.  \\n     - Example:  \\n       ```sql\\n       SELECT $1::INT, $2::STRING FROM @my_stage;  -- Transform before loading.\\n       ```  \\n- **Compute Resources**: Requires a **virtual warehouse** (user-managed) to execute the load.\\n\\n---\\n\\n#### **3. Continuous Data Loading (Snowpipe)**  \\nFor real-time or micro-batch ingestion:  \\n- **Auto-Ingestion**:  \\n  - Snowpipe monitors a stage for new files and loads them automatically.  \\n  - Example:  \\n    ```sql\\n    CREATE PIPE my_pipe \\n    AUTO_INGEST = TRUE\\n    AS \\n    COPY INTO my_table\\n    FROM @my_stage\\n    FILE_FORMAT = (TYPE = JSON);\\n    ```  \\n- **Triggering Manually**:  \\n  ```sql\\n  COPY INTO my_table FROM @my_stage;  -- Manually load new files.\\n  ```  \\n- **Compute Model**: Serverless (Snowflake-managed compute), billed per second.  \\n\\n---\\n\\n#### **4. Handling Semi-Structured Data**  \\nSnowflake supports native handling of JSON, Avro, Parquet, XML, etc.:  \\n- **Direct Querying**:  \\n  Use `VARIANT` type to store semi-structured data in a single column.  \\n  ```sql\\n  SELECT parse_json($1) AS data FROM @my_stage;\\n  ```  \\n- **Schema Detection**:  \\n  Automatically infer schema from semi-structured files:  \\n  ```sql\\n  SELECT GENERATE_COLUMN_DESCRIPTION('INFER_SCHEMA', '@my_stage/*.json');\\n  ```  \\n  Use this to create tables/views with inferred columns.\\n\\n---\\n\\n#### **5. Alternatives to Loading**  \\n- **External Tables**:  \\n  Query data directly from cloud storage without loading into Snowflake tables.  \\n  ```sql\\n  CREATE EXTERNAL TABLE my_external_table\\n  LOCATION = '@my_stage'\\n  FILE_FORMAT = (TYPE = CSV);\\n  ```  \\n  - Useful for read-only analysis of large datasets.\\n\\n---\\n\\n#### **6. Advanced Features**  \\n- **Dynamic Tables**: Automate pipeline orchestration using streams and tasks.  \\n- **Schema Evolution**: Tables can automatically adapt to new columns in incoming data.  \\n- **Unstructured Data**: Process documents (PDFs, images) using **Snowflake Document AI**.\\n\\n---\\n\\n#### **Key Considerations**  \\n- **Permissions**: Ensure roles have access to stages and warehouses.  \\n- **Costs**: Monitor data transfer fees for cross-region/cloud storage.  \\n- **Error Handling**: Use `ON_ERROR` in `COPY INTO` or Snowpipe error logs for troubleshooting.  \\n- **Performance**: Optimize file sizes (e.g., 100MBâ€“500MB per file) and use parallelism for faster loads.\\n\\n---\\n\\n### **Example Workflow**  \\n1. **Stage Data**:  \\n   ```sql\\n   PUT file:///local/data.csv @my_stage;\\n   ```  \\n2. **Load Data**:  \\n   ```sql\\n   COPY INTO sales_data \\n   FROM @my_stage/data.csv \\n   FILE_FORMAT = (TYPE = CSV);\\n   ```  \\n3. **Query Data**:  \\n   ```sql\\n   SELECT * FROM sales_data WHERE amount > 1000;\\n   ```\\n\\nThis structured approach ensures efficient, scalable data ingestion into Snowflake for analytics and machine learning workloads.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1603, 'prompt_tokens': 5804, 'total_tokens': 7407, 'completion_time': 4.099561487, 'completion_tokens_details': None, 'prompt_time': 0.291671538, 'prompt_tokens_details': None, 'queue_time': 0.159189261, 'total_time': 4.391233025}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2344-ba83-7802-8647-7773bc1690db-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 5804, 'output_tokens': 1603, 'total_tokens': 7407})]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "    \"messages\":\"Explain the data loading process in Snowflake.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad195111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='419f4556-1ddd-488a-a40b-9b08979ed1f8'),\n",
       "  AIMessage(content='Machine learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from data. It involves developing algorithms and statistical models that enable computers to perform tasks without being explicitly programmed for them. Instead, these systems improve their performance through experience, by analyzing patterns and making inferences from large datasets. Common applications include image recognition, natural language processing, recommendation systems, and predictive analytics.', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is Machine learning?\" Let me think about how to approach this. First, I need to determine if any of the provided tools are relevant. The tools available are for searching information about Azure Databricks and Snowflake. The user\\'s question is a general definition question about machine learning, not specifically about either of those platforms.\\n\\nLooking at the functions, retriever_vector_db_blog is for Azure Databricks info, and retriever_vector_s_db_blog is for Snowflake. Since the question isn\\'t related to either, using the tools might not provide the best answer. I should check if there\\'s a way the tools could help, maybe if there\\'s a blog post explaining ML in the context of these services. But the user just wants a basic definition. So it\\'s safer to answer directly without using the tools. I\\'ll provide a clear, concise explanation of machine learning as a subset of AI, focusing on learning from data.\\n'}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 220, 'total_tokens': 500, 'completion_time': 0.61760505, 'completion_tokens_details': {'reasoning_tokens': 195}, 'prompt_time': 0.009316497, 'prompt_tokens_details': None, 'queue_time': 0.056844883, 'total_time': 0.626921547}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2347-6ef5-7143-a59e-c96419bd9da9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 220, 'output_tokens': 280, 'total_tokens': 500, 'output_token_details': {'reasoning': 195}})]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d01d6511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELAVANCE---\n",
      "--- Documents are relevant----\n",
      "--- GENERATE RESPONSE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Explain the data loading process in databricks.', additional_kwargs={}, response_metadata={}, id='05ee1a14-53bf-4030-ae03-0d4577ae712b'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking about the data loading process in Databricks. I need to explain how data is loaded into Databricks. First, I should recall the common methods used. Databricks typically uses PySpark or Spark SQL for data ingestion. They might use commands like spark.read to load data from various sources such as CSV, Parquet, or databases.\\n\\nWait, maybe I should check if there\\'s specific information in the provided tools. The user provided a function called retriever_vector_db_blog which is for searching Azure Databricks info. Since the question is about Databricks, I should use that function. The other function is for Snowflake, which isn\\'t relevant here.\\n\\nI should call the retriever_vector_db_blog function with the query \"data loading process\". That should fetch the necessary details. The response will probably include steps like using Spark DataFrame API, supported formats, and maybe integration with storage systems like Azure Blob Storage or Databricks Delta. I need to make sure to use the function to get accurate and up-to-date info rather than relying solely on my existing knowledge.\\n', 'tool_calls': [{'id': 'hnyjn8cpx', 'function': {'arguments': '{\"query\":\"data loading process in Databricks\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 225, 'total_tokens': 483, 'completion_time': 0.552419239, 'completion_tokens_details': {'reasoning_tokens': 224}, 'prompt_time': 0.01016001, 'prompt_tokens_details': None, 'queue_time': 0.159883591, 'total_time': 0.562579249}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2348-2ed1-7850-b6cb-79a2a1c46451-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'data loading process in Databricks'}, 'id': 'hnyjn8cpx', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 225, 'output_tokens': 258, 'total_tokens': 483, 'output_token_details': {'reasoning': 224}}),\n",
       "  ToolMessage(content=\"Azure Databricks provides tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.\\nMachine learning, AI, and data science\\nAzure Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning.\\nLarge language models and generative AI\\n\\nData engineering with Databricks - Azure Databricks | Microsoft Learn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSkip to main content\\n\\t\\t\\n\\n\\t\\t\\tSkip to Ask Learn chat experience\\n\\t\\t\\n\\n\\n\\n\\nThis browser is no longer supported.\\n\\n\\t\\t\\t\\t\\t\\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tDownload Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tMore info about Internet Explorer and Microsoft Edge\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Table of contents \\n\\n\\n\\nExit editor mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nAsk Learn\\n\\n\\n\\n\\n\\nFocus mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\n\\n\\n\\n\\nRead in English\\n\\n\\n\\n\\n\\nAdd\\n\\n\\n\\n\\n\\nAdd to plan\\n\\n\\n\\n\\n\\nEdit\\n\\n\\nShare via\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nx.com\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\nPrint\\n\\nETL and data engineering\\nWhether you're generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models for efficient discovery and use. Azure Databricks combines the power of Apache Spark with Delta and custom tools to provide an unrivaled ETL experience. Use SQL, Python, and Scala to compose ETL logic and orchestrate scheduled job deployment with a few clicks.\\nLakeflow Spark Declarative Pipelines further simplifies ETL by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate data delivery to your specifications.\\n\\nReal-time and streaming analytics\\nAzure Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Lakeflow Spark Declarative Pipelines and Auto Loader. See Structured Streaming concepts.\\nOnline transactional processing\\nLakebase is an online transactional processing (OLTP) database that is fully integrated with the Databricks Data Intelligence Platform. This fully managed Postgres database allows you to create and manage OLTP databases stored in Azure Databricks-managed storage. See What is Lakebase?.\", name='retriever_vector_db_blog', id='026c8dde-73f4-460f-ad1d-75fd7664705f', tool_call_id='hnyjn8cpx'),\n",
       "  AIMessage(content='<think>\\nOkay, let\\'s see. The user asked about the data loading process in Databricks. I need to explain that step by step. First, I remember from the retrieved information that Databricks uses tools like Auto Loader for data ingestion. Auto Loader is good for incremental and idempotent loading from cloud storage. So I should mention how Auto Loader works, maybe how it handles schema evolution and data quality checks.\\n\\nThen, there\\'s the part about using different languages like SQL, Python, or Scala for ETL jobs. Databricks has Delta Lake, which is important for data reliability and performance. I should explain Delta Lake\\'s role here, like ACID transactions and efficient queries.\\n\\nAlso, there\\'s the Lakeflow Spark Declarative Pipelines mentioned. That\\'s a newer feature, so maybe I should include how it simplifies ETL by managing dependencies automatically. Plus, real-time streaming with Structured Streaming and integration with Delta Lake for stream processing.\\n\\nWait, the user might be looking for a comprehensive overview but also practical steps. Maybe outline the main methods: Auto Loader for batch and streaming, Delta for structured data, and tools like Lakeflow for orchestration. Also, mention the use of notebooks and job scheduling.\\n\\nI should check if there are other tools or methods. Oh, right, Databricks has integration with external systems like databases via connectors, and data ingestion from message queues like Kafka. But maybe focus on the core methods first to avoid getting too detailed.\\n\\nAlso, the data loading process steps: ingestion, processing, storage. Each step uses specific tools. Maybe structure the answer with sections like Ingestion Tools, Data Processing, Storage, and Orchestration. Make sure to highlight key features like incremental processing, schema management, and real-time capabilities.\\n\\nNeed to make sure the explanation is clear for someone who might not be familiar with all the terms. Avoid jargon where possible, or explain it. For example, idempotent loading means that the process can be run multiple times without causing duplicates, which is important for reliability.\\n\\nDouble-check the retrieved content for any other relevant points. The Lakebase for OLTP is mentioned, but maybe that\\'s more for transactional processing and not directly related to data loading. Still, mentioning it as part of the ecosystem could be useful.\\n\\nAlright, putting it all together: start with an overview, then break down each component like Auto Loader, Delta Lake, Lakeflow, and Spark Structured Streaming. Conclude with orchestration and scheduling. Make sure each part is explained clearly and how they fit into the overall data loading process in Databricks.\\n</think>\\n\\nThe **data loading process in Databricks** is a streamlined, scalable approach to ingest and process data from diverse sources into a unified data lakehouse architecture. Below is a structured explanation of the key components and workflows involved:\\n\\n---\\n\\n### **1. Data Ingestion Tools**\\nDatabricks provides specialized tools for loading data into the lakehouse (typically stored in **Delta Lake** for reliability and performance).\\n\\n#### **a. Auto Loader (Spark Structured Streaming)**\\n- **Purpose**: Efficiently loads data incrementally and idempotently from cloud storage (e.g., AWS S3, Azure Blob Storage) into Delta tables.\\n- **Key Features**:\\n  - **Schema Evolution**: Automatically infers and evolves schemas as new data arrives.\\n  - **Data Quality Checks**: Includes validation options (e.g., filtering invalid rows).\\n  - **Incremental Processing**: Only processes new data using **Micro-Batch Streaming**.\\n  - **Cloud Compatibility**: Works with object storage systems like S3, ADLS, and GCS.\\n- **Example**:\\n  ```python\\n  df = spark.readStream.format(\"cloudFiles\")\\n                 .option(\"cloudFiles.format\", \"parquet\")\\n                 .load(\"abfss://<container>@<storage-account>.dfs.core.windows.net/<path>\")\\n  ```\\n\\n#### **b. Lakeflow (Declarative Pipelines)**\\n- **Purpose**: Simplifies ETL workflows by declaring data pipelines in SQL or Python.\\n- **Key Features**:\\n  - **Dependency Management**: Auto-generates pipelines based on table relationships.\\n  - **Scalability**: Automatically scales infrastructure for large datasets.\\n  - **Version Control**: Tracks pipeline versions and data lineage.\\n\\n#### **c. External Integrations**\\n- **Databases**: Use JDBC connectors to load data from SQL databases (e.g., MySQL, PostgreSQL).\\n- **Streaming Sources**: Integrate with Kafka, Kinesis, or IoT platforms using **Spark Structured Streaming**.\\n\\n---\\n\\n### **2. Data Processing**\\nOnce data is ingested, Databricks processes it using **Spark** and **Delta Lake**.\\n\\n#### **a. Delta Lake**\\n- **Role**: Provides ACID transactions, schema enforcement, and optimized query performance.\\n- **Key Benefits**:\\n  - **Data Reliability**: Prevents corruption through versioned tables.\\n  - **Efficient Queries**: Uses **Delta Optimized Parquet** for faster reads/writes.\\n  - **Time Travel**: Access historical data versions for auditing or recovery.\\n\\n#### **b. Real-Time Processing**\\n- **Structured Streaming**: Processes streaming data in real-time (e.g., log files, sensor data).\\n- **Integration with Delta**: Stream data is written to Delta tables for unified batch/stream processing.\\n\\n---\\n\\n### **3. Orchestration and Scheduling**\\nDatabricks orchestrates data pipelines using:\\n\\n- **Workflows**: Schedule jobs via the **Databricks Jobs API** or **Databricks CLI**.\\n- **Lakehouse Integration**: Combines batch and streaming workflows into a single pipeline.\\n- **Monitoring**: Track job status, logs, and performance metrics in the Databricks UI.\\n\\n---\\n\\n### **4. Storage and Organization**\\nData is stored in a **Delta Lake**-powered data lakehouse, which supports:\\n- **Unified Storage**: Raw, processed, and curated data in one system.\\n- **Partitioning**: Optimize queries by partitioning data (e.g., by date, region).\\n- **Cloud Storage**: Leverage cost-effective, scalable object storage (e.g., S3, ADLS).\\n\\n---\\n\\n### **5. Advanced Use Cases**\\n- **Machine Learning Pipelines**: Load features into **Feature Store** for ML model training.\\n- **Data Warehousing**: Use **Databricks SQL** to query Delta tables with BI tools.\\n- **OLTP Integration**: **Lakebase** (Postgres-compatible OLTP) for transactional workloads.\\n\\n---\\n\\n### **Summary**\\nDatabricks\\' data loading process combines **Auto Loader**, **Delta Lake**, and orchestration tools to create a robust, scalable pipeline. It supports batch, streaming, and real-time data ingestion, ensuring data is clean, versioned, and ready for analytics or ML workloads. The integration of **Spark** and **Delta** eliminates the need for separate data warehouses, enabling a unified lakehouse architecture.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1422, 'prompt_tokens': 557, 'total_tokens': 1979, 'completion_time': 3.320899436, 'completion_tokens_details': None, 'prompt_time': 0.023162265, 'prompt_tokens_details': None, 'queue_time': 0.061873886, 'total_time': 3.344061701}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2348-405f-7fb0-ae25-70548f565da8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 557, 'output_tokens': 1422, 'total_tokens': 1979})]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "    \"messages\":\"Explain the data loading process in databricks.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630ae50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai-lab (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
